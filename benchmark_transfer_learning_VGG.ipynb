{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing transfer learning on Vgg16 using pyTorch"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi! I am  [Jean-Nicolas Jérémie](https://laurentperrinet.github.io/author/jean-nicolas-jeremie/) and the goal of this notebook is to provide a framework to implement (and experiment with) [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) on deep convolutional neuronal network (DCNN). In a nutshell, [transfer learning](https://www.analyticsvidhya.com/blog/2021/06/transfer-learning-using-vgg16-in-pytorch/) allows to re-use the knowlegde learned on a problem, such as categorizing images from  a large dataset, and apply it to a different (yet related) problem, performing the categorization on a smaller dataset. It is a powerful method as it allows to implement complex task *de novo* quite rapidly (in a few hours) without having to retrain the millions of parameters of a DCNN (which takes days of computations). The basic hypothesis is that it suffices to [re-train the last classification layers](https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch) (the head) while keeping the first layers fixed. Here, these networks teach us also some interesting insights into how living systems may perform such categorization tasks.\n",
        "\n",
        "Based on our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html), we will start from a [VGG16 network](https://pytorch.org/hub/pytorch_vision_vgg/) loaded from the [`torchvision.models` library](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py) and pre-trained on the [Imagenet](http://image-net.org/) dataset wich allows to perform label detection on naturals images for $K = 1000$ labels. Our goal here will be to re-train the last fully-Connected layer of the network to perfom the same task but in a sub-set of $K = 10$ labels from the Imagenet dataset. \n",
        "\n",
        "Moreover, we are going to evaluate different strategies of transfer learning:\n",
        "\n",
        "* VGG General : Substitute the last layer of the pyTorch VGG16 network ($K = 1000$ labels) with a new layer build from a specific subset ($K = 10$ labels).\n",
        "* VGG Linear : Add a new layer build from a specific subset ($K = 10$ labels) after the last Fully-Connected layer of the the pyTorch VGG16 network.\n",
        "* VGG Gray : Same architecture as the VGG General network but trained with grayscale images.\n",
        "* VGG Scale : Same architecture as the VGG General network but trained with images of different size.\n",
        "\n",
        "In this notebook, I will use the [pyTorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook was done during a master 2 internship at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2020-06-26_fast_and_curious.git).\n",
        "\n",
        "<!-- TEASER_END -->\n",
        "\n",
        "In our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html), as the VGG16 network was first trained on the entire dataset of $K=1000$ labels, and in order to recover the classification confidence predicted by the model according to the specific subset of classes ($K = 10$ labels) on which it is tested, the output `softmax` mathematical function of the last layer of the network was slightly changed. By assuming that we know *a priori* that the image belongs to one (and only one) category from the sub-set the probabilities obtained would correspond to a confidence of classification discriminating only the classes of interest and can be compared to a chance level of $1 /K$. This creates another network (which is not retrained) directly based on VGG:\n",
        "\n",
        "* VGG Subset : Just consider the specific subset ($K = 10$ labels) from the last layer of the pyTorch VGG16 network ($K = 1000$ labels).\n",
        "\n",
        "This notebook aims in addition to test this hypothesis. Our use case consists of measuring whether there are differences in the likelihood of these networks during an image recognition task on a sub-set of $1000$ classes of the `ImageNet` library, with $K = 10$ (experiment 1). Additionally, we will implement some image transformations as up/down-sampling (experiment 2) or transforming to grayscale (experiment 3) to quantify their influence on the accuracy and computation time of each network.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Some useful links :*\n",
        "\n",
        "* https://jaketae.github.io/study/pytorch-vgg/\n",
        "* https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
        "* https://www.kaggle.com/paultimothymooney/detect-retina-damage-from-oct-images/notebook\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Transfer_learning\n",
        "* https://github.com/laurentperrinet/ImageNet-Datasets-Downloader/tree/8d1c0925b5512f48978177a76e7b851ff40acb7b"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first install requirements"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "!make clean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ltr"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# uncommment to re-run training\n",
        "#%rm -fr models\n",
        "%mkdir -p DCNN_transfer_learning\n",
        "%mkdir -p results\n",
        "%mkdir -p models"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization of the libraries/variables\n",
        "\n",
        "Our coding strategy is to build up a small library as a package of scripts in the `DCNN_benchmark` folder and to run all calls to that library from this notebook. This follows our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html) in which we benchmarked various DCNNs and which allowed us to select VGG16 as a good compromise between performance and complexity.\n",
        "\n",
        "First of all, a `init.py` script defines all our usefull variables like the new labels to learn, the number of training images or the root folder to use. Also, we import libraries to train the different networks and display the results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'DCNN_transfer_learning/init.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "# Importing libraries\n",
        "import torch\n",
        "import argparse\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('xtick', labelsize=18)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=18)    # fontsize of the tick labels\n",
        "import numpy as np\n",
        "#from numpy import random\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "from time import strftime, gmtime\n",
        "datetag = strftime(\"%Y-%m-%d\", gmtime())\n",
        "datetag = '2021-10-26'\n",
        "\n",
        "HOST, device = os.uname()[1], torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HOST, device = 'inv-ope-de06', torch.device(\"cuda\")\n",
        "\n",
        "    \n",
        "# to store results\n",
        "import pandas as pd\n",
        "\n",
        "def arg_parse():\n",
        "    DEBUG = 25\n",
        "    DEBUG = 1\n",
        "    parser = argparse.ArgumentParser(description='DCNN_transfer_learning/init.py set root')\n",
        "    parser.add_argument(\"--root\", dest = 'root', help = \"Directory containing images to perform the training\",\n",
        "                        default = 'data', type = str)\n",
        "    parser.add_argument(\"--folders\", dest = 'folders', help =  \"Set the training, validation and testing folders relative to the root\",\n",
        "                        default = ['test', 'val', 'train'], type = list)\n",
        "    parser.add_argument(\"--N_images\", dest = 'N_images', help =\"Set the number of images per classe in the train folder\",\n",
        "                        default = [400//DEBUG, 200//DEBUG, 800//DEBUG], type = list)\n",
        "    parser.add_argument(\"--HOST\", dest = 'HOST', help = \"Set the name of your machine\",\n",
        "                    default=HOST, type = str)\n",
        "    parser.add_argument(\"--datetag\", dest = 'datetag', help = \"Set the datetag of the result's file\",\n",
        "                    default = datetag, type = str)\n",
        "    parser.add_argument(\"--image_size\", dest = 'image_size', help = \"Set the default image_size of the input\",\n",
        "                    default = 256)\n",
        "    parser.add_argument(\"--image_sizes\", dest = 'image_sizes', help = \"Set the image_sizes of the input for experiment 2 (downscaling)\",\n",
        "                    default = [64, 128, 256, 512], type = list)\n",
        "    parser.add_argument(\"--num_epochs\", dest = 'num_epochs', help = \"Set the number of epoch to perform during the traitransportationning phase\",\n",
        "                    default = 200//DEBUG)\n",
        "    parser.add_argument(\"--batch_size\", dest = 'batch_size', help=\"Set the batch size\", default = 32)\n",
        "    parser.add_argument(\"--lr\", dest = 'lr', help=\"Set the learning rate\", default = 0.0001)\n",
        "    parser.add_argument(\"--momentum\", dest = 'momentum', help=\"Set the momentum\", default = 0.9)\n",
        "    parser.add_argument(\"--beta2\", dest = 'beta2', help=\"Set the second momentum - use zero for SGD\", default = 0.)\n",
        "    parser.add_argument(\"--subset_i_labels\", dest = 'subset_i_labels', help=\"Set the labels of the classes (list of int)\",\n",
        "                    default = [945, 513, 886, 508, 786, 310, 373, 145, 146, 396], type = list)\n",
        "    parser.add_argument(\"--class_loader\", dest = 'class_loader', help = \"Set the Directory containing imagenet downloaders class\",\n",
        "                        default = 'imagenet_label_to_wordnet_synset.json', type = str)\n",
        "    parser.add_argument(\"--url_loader\", dest = 'url_loader', help = \"Set the file containing imagenet urls\",\n",
        "                        default = 'Imagenet_urls_ILSVRC_2016.json', type = str)\n",
        "    parser.add_argument(\"--model_path\", dest = 'model_path', help = \"Set the path to the pre-trained model\",\n",
        "                        default = 'models/re-trained_', type = str)\n",
        "    parser.add_argument(\"--model_names\", dest = 'model_names', help = \"Modes for the new trained networks\",\n",
        "                        default = ['vgg16_lin', 'vgg16_gen', 'vgg16_scale', 'vgg16_gray', ], type = list)\n",
        "    return parser.parse_args()\n",
        "\n",
        "args = arg_parse()\n",
        "datetag = args.datetag\n",
        "json_fname = os.path.join('results', datetag + '_config_args.json')\n",
        "load_parse = False # False to custom the config\n",
        "\n",
        "if load_parse:\n",
        "    with open(json_fname, 'rt') as f:\n",
        "        print(f'file {json_fname} exists: LOADING')\n",
        "        override = json.load(f)\n",
        "        args.__dict__.update(override)\n",
        "else:\n",
        "    print(f'Creating file {json_fname}')\n",
        "    with open(json_fname, 'wt') as f:\n",
        "        json.dump(vars(args), f, indent=4)\n",
        "    \n",
        "# matplotlib parameters\n",
        "colors = ['b', 'r', 'k', 'g', 'm']\n",
        "fig_width = 20\n",
        "phi = (np.sqrt(5)+1)/2 # golden ratio for the figures :-)\n",
        "\n",
        "#to plot & display \n",
        "def pprint(message): #display function\n",
        "    print('-'*len(message))\n",
        "    print(message)\n",
        "    print('-'*len(message))\n",
        "    \n",
        "#DCCN training\n",
        "print('On date', args.datetag, ', Running benchmark on host', args.HOST, ' with device', device.type)\n",
        "\n",
        "# Labels Configuration\n",
        "N_labels = len(args.subset_i_labels)\n",
        "\n",
        "paths = {}\n",
        "N_images_per_class = {}\n",
        "for folder, N_image in zip(args.folders, args.N_images):\n",
        "    paths[folder] = os.path.join(args.root, folder) # data path\n",
        "    N_images_per_class[folder] = N_image\n",
        "    os.makedirs(paths[folder], exist_ok=True)\n",
        "    \n",
        "with open(args.class_loader, 'r') as fp: # get all the classes on the data_downloader\n",
        "    imagenet = json.load(fp)\n",
        "\n",
        "# gathering labels\n",
        "labels = []\n",
        "class_wnids = []\n",
        "reverse_id_labels = {}\n",
        "for a, img_id in enumerate(imagenet):\n",
        "    reverse_id_labels[str('n' + (imagenet[img_id]['id'].replace('-n','')))] = imagenet[img_id]['label'].split(',')[0]\n",
        "    labels.append(imagenet[img_id]['label'].split(',')[0])\n",
        "    if int(img_id) in args.subset_i_labels:\n",
        "        class_wnids.append('n' + (imagenet[img_id]['id'].replace('-n','')))    \n",
        "        \n",
        "# a reverse look-up-table giving the index of a given label (within the whole set of imagenet labels)\n",
        "reverse_labels = {}\n",
        "for i_label, label in enumerate(labels):\n",
        "    reverse_labels[label] = i_label\n",
        "# a reverse look-up-table giving the index of a given i_label (within the sub-set of classes)\n",
        "reverse_subset_i_labels = {}\n",
        "for i_label, label in enumerate(args.subset_i_labels):\n",
        "    reverse_subset_i_labels[label] = i_label\n",
        "    \n",
        "# a reverse look-up-table giving the label of a given index in the last layer of the new model (within the sub-set of classes)\n",
        "subset_labels = []\n",
        "pprint('List of Pre-selected classes : ')\n",
        "# choosing the selected classes for recognition\n",
        "for i_label, id_ in zip(args.subset_i_labels, class_wnids) : \n",
        "    subset_labels.append(labels[i_label])\n",
        "    print('-> label', i_label, '=', labels[i_label], '\\nid wordnet : ', id_)\n",
        "subset_labels.sort()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname} "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the `train` & `val` dataset\n",
        "\n",
        "In the `dataset.py`, we use an archive of the Imagenet fall 2011 urls to populate datasets based on the pre-selected classes listed in the `DCNN_benchmark/init.py` file. The following script is inspired by [previous work](https://github.com/laurentperrinet/ImageNet-Datasets-Downloader/) in our group."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "paths, args.folders"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "imageio.imread?"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "imageio.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Crozet_-_Manchots.jpg/800px-Crozet_-_Manchots.jpg').shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'DCNN_transfer_learning/dataset.py'"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-26T19:05:14.522Z",
          "iopub.execute_input": "2021-10-26T19:05:14.629Z",
          "iopub.status.idle": "2021-10-26T19:05:14.704Z",
          "shell.execute_reply": "2021-10-26T19:05:14.773Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "from DCNN_transfer_learning.init import *  \n",
        "verbose = False\n",
        "\n",
        "with open(args.url_loader) as json_file:\n",
        "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)\n",
        "\n",
        "def clean_list(list_dir, patterns=['.DS_Store']):\n",
        "    for pattern in patterns:\n",
        "        if pattern in list_dir: list_dir.remove('.DS_Store')\n",
        "    return list_dir\n",
        "\n",
        "import imageio\n",
        "def get_image(img_url, timeout=3., min_content=5000, verbose=verbose):\n",
        "    try:\n",
        "        img_resp = imageio.imread(img_url)\n",
        "        if verbose : print(f\"Success with url {img_url}\")\n",
        "        return img_resp\n",
        "    except Exception as e:\n",
        "        if verbose : print(f\"Failed with {e} for url {img_url}\")\n",
        "        return False # did not work\n",
        "\n",
        "import hashlib # jah.\n",
        "# root folder\n",
        "os.makedirs(args.root, exist_ok=True)\n",
        "# train, val and test folders\n",
        "for folder in args.folders : \n",
        "    os.makedirs(paths[folder], exist_ok=True)\n",
        "    \n",
        "list_urls = {}\n",
        "list_img_name_used = {}\n",
        "for class_wnid in class_wnids:\n",
        "    list_urls[class_wnid] =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
        "    np.random.shuffle(list_urls[class_wnid])\n",
        "    list_img_name_used[class_wnid] = []\n",
        "\n",
        "    # a folder per class in each train, val and test folder\n",
        "    for folder in args.folders : \n",
        "        class_name = reverse_id_labels[class_wnid]\n",
        "        class_folder = os.path.join(paths[folder], class_name)\n",
        "        os.makedirs(class_folder, exist_ok=True)\n",
        "        list_img_name_used[class_wnid] += clean_list(os.listdir(class_folder)) # join two lists\n",
        "    \n",
        "# train, val and test folders\n",
        "for folder in args.folders : \n",
        "    print(f'Folder \\\"{folder}\\\"')\n",
        "\n",
        "    filename = f'results/{datetag}_dataset_{folder}_{args.HOST}.json'\n",
        "    columns = ['img_url', 'img_name', 'is_flickr', 'dt', 'worked', 'class_wnid', 'class_name']\n",
        "    if os.path.isfile(filename):\n",
        "        df_dataset = pd.read_json(filename)\n",
        "    else:\n",
        "        df_dataset = pd.DataFrame([], columns=columns)\n",
        "\n",
        "    for class_wnid in class_wnids:\n",
        "        class_name = reverse_id_labels[class_wnid]\n",
        "        print(f'Scraping images for class \\\"{class_name}\\\"')\n",
        "        class_folder = os.path.join(paths[folder], class_name)\n",
        "        while (len(clean_list(os.listdir(class_folder))) < N_images_per_class[folder]) and (len(list_urls[class_wnid]) > 0):\n",
        "\n",
        "            # pick and remove element from shuffled list \n",
        "            img_url = list_urls[class_wnid].pop()\n",
        "            \n",
        "            if len(df_dataset[df_dataset['img_url']==img_url])==0 : # we have not yet tested this URL yet\n",
        "\n",
        "                # Transform URL into filename\n",
        "                # https://laurentperrinet.github.io/sciblog/posts/2018-06-13-generating-an-unique-seed-for-a-given-filename.html\n",
        "                img_name = hashlib.sha224(img_url.encode('utf-8')).hexdigest() + '.png'\n",
        "\n",
        "                if img_url.split('.')[-1] in ['jpe', 'gif']:\n",
        "                    if verbose: print('Bad extension for the img_url', img_url)\n",
        "                    worked, dt = False, 0.\n",
        "                # make sure it was not used in other folders\n",
        "                elif not (img_name in list_img_name_used[class_wnid]):\n",
        "                    tic = time.time()\n",
        "                    img_content = get_image(img_url, verbose=verbose)\n",
        "                    dt = time.time() - tic\n",
        "                    \n",
        "                    worked = img_content is not False\n",
        "                    if worked:\n",
        "                        if verbose : print('Good URl, now saving', img_url, ' in', class_folder, ' as', img_name)\n",
        "                        imageio.imsave(os.path.join(class_folder, img_name), img_content, format='png')\n",
        "                        list_img_name_used[class_wnid].append(img_name)\n",
        "                df_dataset.loc[len(df_dataset.index)] = {'img_url':img_url, 'img_name':img_name, 'is_flickr':1 if 'flickr' in img_url else 0, 'dt':dt,\n",
        "                                'worked':worked, 'class_wnid':class_wnid, 'class_name':class_name}\n",
        "                df_dataset.to_json(filename)\n",
        "                print(f'\\r{len(clean_list(os.listdir(class_folder)))} / {N_images_per_class[folder]}', end='\\n' if verbose else '', flush=not verbose)\n",
        "            #print('\\n')\n",
        "        if (len(clean_list(os.listdir(class_folder))) < N_images_per_class[folder]) and (len(list_urls[class_wnid]) == 0): \n",
        "            print('Not enough working url to complete the dataset') \n",
        "    \n",
        "    df_dataset.to_json(filename)\n",
        "\n",
        "\n",
        "if False: \n",
        "    \n",
        "    # replace the file with that URLs that worked - removes the ones that failed\n",
        "    print(f'Replacing file {args.url_loader}')\n",
        "    with open(args.url_loader, 'wt') as f:\n",
        "        json.dump(Imagenet_urls_ILSVRC_2016, f, indent=4)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting DCNN_transfer_learning/dataset.py\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-26T19:05:18.249Z",
          "iopub.execute_input": "2021-10-26T19:05:18.287Z",
          "iopub.status.idle": "2021-10-26T19:05:18.386Z",
          "shell.execute_reply": "2021-10-26T19:05:18.435Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "list_img_name_used[class_wnid]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_dataset.index)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "list_img_name_used['n02056570']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset[df_dataset['img_url']=='http://www.cdli.ca/CITE/emperor_penguins.gif']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open(args.url_loader) as json_file:\n",
        "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for class_wnid in class_wnids:\n",
        "        list_urls =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
        "        np.random.shuffle(list_urls)    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "list_urls = {}\n",
        "for class_wnid in class_wnids:\n",
        "    list_urls[class_wnid] =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
        "    np.random.shuffle(list_urls[class_wnid])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class_wnid"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "list_urls[class_wnid][0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "list_urls[class_wnid].pop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "paths, os.listdir('data/'), clean_list(os.listdir(paths[folder])), class_wnids"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in args.folders:\n",
        "    for class_wnid in class_wnids:\n",
        "        class_name = reverse_id_labels[class_wnid]\n",
        "        class_folder = os.path.join(paths[folder], class_name)\n",
        "        list_dir = os.listdir(class_folder)\n",
        "        print(folder, class_name, len(clean_list(list_dir)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Imagenet_urls_ILSVRC_2016.keys()\n",
        "len(Imagenet_urls_ILSVRC_2016['n03085013'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some statistics for the scrapped images:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'results/2021-10-20_dataset_test_inv-ope-de06.json'\n",
        "df_dataset = pd.read_json(filename)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in args.folders : \n",
        "    filename = f'results/{datetag}_dataset_{folder}_{args.HOST}.json'\n",
        "    if os.path.isfile(filename):\n",
        "        df_dataset = pd.read_json(filename)\n",
        "\n",
        "        df_type = pd.DataFrame({'urls_type': [len(df_dataset[df_dataset['is_flickr']==1]), \n",
        "                                              len(df_dataset[df_dataset['is_flickr']==0])]},\n",
        "                          index=['is_flickr', 'not_flikr'])\n",
        "        df_flikr = pd.DataFrame({'not_flikr': [df_dataset[df_dataset['is_flickr']==0]['worked'].sum(), \n",
        "                                               (len(df_dataset[df_dataset['is_flickr']==0]) - df_dataset[df_dataset['is_flickr']==0]['worked'].sum())],\n",
        "                                 'is_flickr': [df_dataset[df_dataset['is_flickr']==1]['worked'].sum(), \n",
        "                                               (len(df_dataset[df_dataset['is_flickr']==1]) - df_dataset[df_dataset['is_flickr']==1]['worked'].sum())]},\n",
        "                                  index=['worked', 'not_working'])\n",
        "        df_all = pd.DataFrame({'url': [len(df_dataset[df_dataset['worked']==1]), len(df_dataset[df_dataset['worked']==0])]},\n",
        "                          index=['worked', 'not_working'])\n",
        "\n",
        "        plot_type = df_type.plot.pie(y='urls_type', figsize=(5, 5), labeldistance=None)\n",
        "        plot_type.set_title('Stats for the folder '+ folder + ' (' + str(len(df_dataset)) + ' attempts) :', size = 18)\n",
        "        if not len(df_dataset[df_dataset['is_flickr']==0]) == 0 or len(df_dataset[df_dataset['is_flickr']==1]) == 0: \n",
        "            plot_flickr = df_flikr.plot.pie(subplots=True, figsize=(11, 6), labeldistance=None)\n",
        "        plot_all = df_all.plot.pie(y='url', figsize=(5, 5), labeldistance=None)\n",
        "    else:\n",
        "        print(f'The file {filename} is not available...')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : show one representative image from each class"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning and dataset config\n",
        "\n",
        "In the `model.py` scrip,t we first define the `transform` functions for the datasets. To perform image augmentation, we apply the pyTorch `AutoAugment` function to the `train` and `val` dataset. Then, we load the pretrained models and store them in memory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'DCNN_transfer_learning/model.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "from DCNN_transfer_learning.init import *\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "\n",
        "# normalization used to train VGG\n",
        "# see https://pytorch.org/hub/pytorch_vision_vgg/\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "transforms_norm = transforms.Normalize(mean=mean, std=std) # to normalize colors on the imagenet dataset\n",
        "\n",
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "from scipy import stats\n",
        "from scipy.special import logit, expit\n",
        "\n",
        "# VGG-16 datasets initialisation\n",
        "def datasets_transforms(image_size=args.image_size, p=0, num_workers=1, batch_size=args.batch_size, **kwargs):\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize((int(image_size), int(image_size))),\n",
        "            transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugment\n",
        "            transforms.RandomGrayscale(p=p),\n",
        "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
        "            transforms_norm ]),\n",
        "\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize((int(image_size), int(image_size))),\n",
        "            transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugment\n",
        "            transforms.RandomGrayscale(p=p),\n",
        "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
        "            transforms_norm ]),\n",
        "\n",
        "        'test': transforms.Compose([\n",
        "            transforms.Resize((int(image_size), int(image_size))),\n",
        "            transforms.RandomGrayscale(p=p),\n",
        "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
        "            transforms_norm ]),\n",
        "    }\n",
        "    #print(paths)\n",
        "    \n",
        "\n",
        "    image_datasets = {\n",
        "        folder: datasets.ImageFolder(\n",
        "            paths[folder], \n",
        "            transform=data_transforms[folder]\n",
        "        )\n",
        "        for folder in args.folders\n",
        "    }\n",
        "\n",
        "    dataloaders = {\n",
        "        folder: torch.utils.data.DataLoader(\n",
        "            image_datasets[folder], batch_size=batch_size,\n",
        "            shuffle=True, num_workers=num_workers\n",
        "        )\n",
        "        for folder in args.folders\n",
        "    }\n",
        "\n",
        "    dataset_sizes = {folder: len(image_datasets[folder]) for folder in args.folders}\n",
        "\n",
        "    return dataset_sizes, dataloaders, image_datasets, data_transforms\n",
        "\n",
        "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size)\n",
        "\n",
        "for folder in args.folders : print(f\"Loaded {dataset_sizes[folder]} images under {folder}\")\n",
        "class_names = image_datasets['train'].classes\n",
        "print(\"Classes: \", image_datasets['train'].classes)\n",
        "n_output = len(os.listdir(paths['train']))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training process\n",
        "\n",
        "Finaly, we implement the training process in `experiment_train.py`, using a classic training script with pyTorch. For further statistical analyses, we extract factors (like the accuracy and loss) within a `pandas` object (a `DataFrame`). "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'experiment_train.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "from DCNN_transfer_learning.model import *\n",
        "\n",
        "def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, beta2=args.beta2, log_interval=100, **kwargs):\n",
        "    \n",
        "    model.to(device)\n",
        "    if beta2 > 0.: \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(momentum, beta2)) #, amsgrad=amsgrad)\n",
        "    else:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum) # to set training variables\n",
        "\n",
        "    df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val', 'device_type']) \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_train = 0\n",
        "        acc_train = 0\n",
        "        for i, (images, labels) in enumerate(dataloaders['train']):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            acc_train += torch.sum(preds == labels.data)\n",
        "            \n",
        "        avg_loss = loss_train / dataset_sizes['train']\n",
        "        avg_acc = acc_train / dataset_sizes['train']\n",
        "           \n",
        "        with torch.no_grad():\n",
        "            loss_val = 0\n",
        "            acc_val = 0\n",
        "            for i, (images, labels) in enumerate(dataloaders['val']):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss_val += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "                acc_val += torch.sum(preds == labels.data)\n",
        "        \n",
        "            avg_loss_val = loss_val / dataset_sizes['val']\n",
        "            avg_acc_val = acc_val / dataset_sizes['val']\n",
        "        \n",
        "        df_train.loc[epoch] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':float(avg_acc),\n",
        "                               'avg_loss_val':avg_loss_val, 'avg_acc_val':float(avg_acc_val), 'device_type':device.type}\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}\")\n",
        "\n",
        "    model.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "    return model, df_train\n",
        "\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Training and saving the network\n",
        "\n",
        "models_vgg = {}\n",
        "opt = {}\n",
        "#df_train = {}\n",
        "\n",
        "models_vgg['vgg'] = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "# Downloading the model\n",
        "model_filenames = {}\n",
        "for model_name in args.model_names:\n",
        "    model_filenames[model_name] = args.model_path + model_name + '.pt'\n",
        "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
        "\n",
        "    models_vgg[model_name] = torchvision.models.vgg16(pretrained=True)\n",
        "    # TODO : compare with full learning\n",
        "    # Freeze training for all layers\n",
        "    # Newly created modules have require_grad=True by default\n",
        "    for param in models_vgg[model_name].features.parameters():\n",
        "        param.require_grad = False \n",
        "\n",
        "    if model_name == 'vgg16_lin':\n",
        "        num_features = models_vgg[model_name].classifier[-1].out_features\n",
        "        features = list(models_vgg[model_name].classifier.children())\n",
        "        features.extend([nn.Linear(num_features, n_output)]) # Adding one layer on top of last layer\n",
        "        models_vgg[model_name].classifier = nn.Sequential(*features)\n",
        "\n",
        "    else : \n",
        "        num_features = models_vgg[model_name].classifier[-1].in_features\n",
        "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
        "        features.extend([nn.Linear(num_features, n_output)]) # Add our layer with 10 outputs\n",
        "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "    if os.path.isfile(model_filenames[model_name]):\n",
        "        print(\"Loading pretrained model for..\", model_name, ' from', model_filenames[model_name])\n",
        "        #print(\"Resume_training : \", resume_training)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name])) #on GPU\n",
        "        else:\n",
        "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name], map_location=torch.device('cpu'))) #on CPU\n",
        "\n",
        "    else:\n",
        "        print(\"Re-training pretrained model...\", model_filenames[model_name])\n",
        "        since = time.time()\n",
        "\n",
        "        p = 1 if model_name == 'vgg16_gray' else 0\n",
        "        if model_name =='vgg16_scale':\n",
        "            df_train = None\n",
        "            for image_size_ in args.image_sizes: # starting with low resolution images \n",
        "                print(f\"Traning {model_name}, image_size = {image_size_}, p (Grayscale) = {p}\")\n",
        "                (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=image_size_, p=p)\n",
        "                models_vgg[model_name], df_train_ = train_model(models_vgg[model_name], num_epochs=args.num_epochs//len(args.image_sizes),\n",
        "                                                             dataloaders=dataloaders)\n",
        "                df_train = df_train_ if df_train is None else df_train.append(df_train_, ignore_index=True)\n",
        "        else :\n",
        "            print(f\"Traning {model_name}, image_size = {args.image_size}, p (Grayscale) = {p}\")\n",
        "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=p)\n",
        "            models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
        "                                                        dataloaders=dataloaders)\n",
        "        torch.save(models_vgg[model_name].state_dict(), model_filenames[model_name])\n",
        "        df_train.to_json(filename)\n",
        "        elapsed_time = time.time() - since\n",
        "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
        "        print()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we display both average accuracy and loss during the training phase and during the validation one : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'vgg16_lin'\n",
        "filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
        "%ls {filename}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in args.model_names:\n",
        "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
        "    df_train = pd.read_json(filename)\n",
        "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi//2))\n",
        "    # plt.xticks(fontsize=18)\n",
        "    # plt.yticks(fontsize=18)\n",
        "    ax = df_train['avg_loss'].plot(lw=2, marker='.', markersize=10)\n",
        "    ax = df_train['avg_loss_val'].plot(lw=2, marker='.', markersize=10)\n",
        "    ax.legend([\"avg_loss\", \"avg_loss_val\"], fontsize=18);\n",
        "    ax.set_xlabel(\"Epoch\", size=18)\n",
        "    ax.set_ylabel(\"Loss value\", size=18)\n",
        "    ax.set_ylim(0, 1.2)\n",
        "    axs.set_title(f'Average values of the loss by epoch : {filename}' , size = 20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in args.model_names:\n",
        "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
        "    df_train = pd.read_json(filename)\n",
        "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi//2))\n",
        "    # plt.xticks(fontsize=18)\n",
        "    # plt.yticks(fontsize=18)\n",
        "    ax = df_train['avg_acc'].plot(lw=2, marker='.', markersize=10)\n",
        "    ax = df_train['avg_acc_val'].plot(lw=2, marker='.', markersize=10)\n",
        "    ax.legend([\"avg_acc\", \"avg_acc_val\"], fontsize=18);\n",
        "    ax.set_xlabel(\"Epoch\", size=18)\n",
        "    ax.set_ylabel(\"Accuracy value\", size=18)\n",
        "    ax.set_ylim(0.60, 1)\n",
        "    axs.set_title(f'Average values of the accuracy by epoch : {filename}' , size = 20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Image processing and recognition for differents labels \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The networks are now ready for a quantitative evaluation. The second part of this notebook offers a comparison between : \n",
        "\n",
        "- A pre-trained image recognition's networks, here VGG, trained on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for $1000$ labels, taken from the `torchvision.models` library\n",
        "\n",
        "- And four re-trained version of the same network VGG16 based on a reduced Imagenet dataset wich allows to focus on naturals images from $10$ labels.\n",
        "\n",
        "For further statistical analyses, we extract these differents factors (like the accuracy and the processing time for differents datasets at differents resolution) in a `pandas` object. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'experiment_basic.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "#import model's script and set the output file\n",
        "from DCNN_transfer_learning.model import *\n",
        "#from experiment_train import *\n",
        "filename = f'results/{datetag}_results_1_{args.HOST}.json'\n",
        "print(f'{filename=}')\n",
        "def main():\n",
        "    if os.path.isfile(filename):\n",
        "        df = pd.read_json(filename)\n",
        "    else:\n",
        "        i_trial = 0\n",
        "        df = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device_type', 'top_1']) \n",
        "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
        "        \n",
        "        for i_image, (data, label) in enumerate(dataloaders['test']):            \n",
        "            data, label = data.to(device), label.to(device)\n",
        "            \n",
        "            for model_name in models_vgg.keys():\n",
        "                model = models_vgg[model_name]\n",
        "                model = model.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
        "                    tic = time.time()\n",
        "                    out = model(data).squeeze(0)\n",
        "                    _, indices = torch.sort(out, descending=True)\n",
        "                    if model_name == 'vgg' : # our previous work\n",
        "                        top_1 = labels[indices[0]]\n",
        "                        percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
        "                        perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
        "                    else :\n",
        "                        top_1 = subset_labels[indices[0]] \n",
        "                        percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
        "                        perf_ = percentage[label].item()\n",
        "                    elapsed_time = time.time() - tic\n",
        "                    \n",
        "                print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {elapsed_time:.3f} seconds, best confidence for : {top_1}')\n",
        "                df.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
        "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
        "                                   'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'device_type':device.type, 'top_1':top_1}\n",
        "                i_trial += 1\n",
        "        df.to_json(filename)\n",
        "\n",
        "main()    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we collect our results, we can already display all the data in a table "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'results/{datetag}_results_1_{args.HOST}.json'\n",
        "filename"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(filename)\n",
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy, Precision, Recall\n",
        "\n",
        "We now compute the top-1 accuracy (which is a metric that describes how the model performs across all classes, here top 1 because we only take the best likelihood at the outputof the networks), the precision (which reflects how reliable the model is in classifying samples as Positive) and the recall (which measures the model's ability to detect Positive samples) of each networks. We use the [sklearn librairy](https://sklearn.org/index.html) to perform this [analysis](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_precision = pd.DataFrame({model_name: {subset_label: precision_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
        "                                                                  df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"],\n",
        "                                                                 average='micro')\n",
        "                                    for subset_label in subset_labels} \n",
        "                       for model_name in models_vgg.keys()})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df_precision.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title('Precision for each models - experiment 1', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_xlabel('Models', size=14);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : use the f1-score everywhere\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_f1_score = pd.DataFrame({model_name: {subset_label: f1_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
        "                                                                df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"],\n",
        "                                                                average='micro')\n",
        "                                    for subset_label in subset_labels} \n",
        "                       for model_name in models_vgg.keys()})\n",
        "\n",
        "ax = df_f1_score.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title('F1-score for each models - experiment 1', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
        "                                                                 df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"])\n",
        "                                    for subset_label in subset_labels} \n",
        "                       for model_name in models_vgg.keys()})\n",
        "\n",
        "ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title('Accuracy top_1 : for each models - experiment 1', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_xlabel('Models', size=14);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.DataFrame({'accuracy': [accuracy_score(df[df['model']==model_name][\"top_1\"], df[df['model']==model_name][\"label\"]) for model_name in models_vgg.keys()]}, index=models_vgg.keys())\n",
        "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
        "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=14, fmt='%.3f')\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title('Average accuracy top_1 : for each models - experiment 1', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_xlabel('Models', size=14);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation time\n",
        "\n",
        "A display of the differents computation time of each models on the same dataset for the sequence of trials :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
        "    ax.set_ylabel('Frequency', fontsize=14)\n",
        "    df[df['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=model_name,ax=ax, color=color, density=True)\n",
        "    ax.legend(loc='upper left', fontsize=20)\n",
        "    ax.set_xlim(df['time'].quantile(.01), df['time'].quantile(.99))\n",
        "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
        "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification likelihood\n",
        "\n",
        "This graph shows the frequency of the logit of the classification likelihood for our four models and the pyTorch VGG16 model. The classification likelihood represent a predicted likelihood probability of detection for a given label at the output of the network. As most of them are close either to 100 as to 0, we applied a logit function to make the difference between these networks more obvious."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
        "    ax.set_ylabel('Frequency', fontsize=14)\n",
        "    (logit(df[df['model']==model_name]['perf']/100)/np.log2(10)).plot.hist(bins=np.linspace(0, 5, 150), lw=1, label=model_name, ax=ax, color=color, density=True)\n",
        "    #df[df['model']==model_name]['perf'].plot.hist(bins=np.linspace(99.95, 100, 150), lw=1, label=model_name, ax=ax, color=color, density=True)\n",
        "    ax.legend(loc='upper left', fontsize=20)\n",
        "    ax.set_ylim(0, 5)\n",
        "    # ax.tick_params(axis='x', labelsize=14)\n",
        "    # ax.tick_params(axis='y', labelsize=14)\n",
        "axs[-1].set_xlabel('Classification likelihood (ban)', size=18)\n",
        "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : change name of variable perf to likelihood ??\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO \n",
        "- voir si on peut donner la valeur du logit en hartley base 10 https://en.wikipedia.org/wiki/Hartley_(unit) tracer le chance level\n",
        "- show success of categorization vs likelihood / odd-ratio of label vs rest"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image display\n",
        "\n",
        "Here we display the 64 *worsts* Classification likelihood's, all model combined : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N_image_i = 8\n",
        "N_image_j = 8\n",
        "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
        "for i_image, idx in enumerate(df[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
        "    ax = axs[i%N_image_i][i_image//N_image_i]\n",
        "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df.loc[idx]['i_image']][0]))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlabel(df.loc[idx]['label'] + ' | ' + df.loc[idx]['model'], color='g')\n",
        "    perf_ = df.loc[idx]['perf']\n",
        "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summary\n",
        "\n",
        "To make it even clearer we extracted a specific mean for each models : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean accuracy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    mean_acc = (df[df['model']==model_name][\"top_1\"] == df[df['model']==model_name][\"label\"]).mean()\n",
        "    print(f'For the {model_name} model, the mean accuracy = {mean_acc*100:.4f} %' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification likelihood's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf = np.mean(df[df['model']==model_name][\"perf\"])\n",
        "    print(f'For the {model_name} model, the mean clasification likelihood = {med_perf:.4f} %' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation time 's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf = np.mean(df[df['model']==model_name][\"time\"])\n",
        "    print(f'For the {model_name} model, the mean computation time = {med_perf:.5f} s')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frame per second's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf = np.mean(df[df['model']==model_name][\"fps\"])\n",
        "    print(f'For the {model_name} model, the mean fps = {med_perf:.3f} Hz' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: Image processing and recognition for differents resolutions :\n",
        "\n",
        "Let's now study that same likelihood indicators at different image resolutions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'experiment_downsample.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "#import model's script and set the output file\n",
        "from DCNN_transfer_learning.model import *\n",
        "filename = f'results/{datetag}_results_2_{args.HOST}.json'\n",
        "\n",
        "def main():\n",
        "    if os.path.isfile(filename):\n",
        "        df_downsample = pd.read_json(filename)\n",
        "    else:\n",
        "        i_trial = 0\n",
        "        df_downsample = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'image_size', 'filename', 'device_type', 'top_1']) \n",
        "        # image preprocessing\n",
        "        for image_size_ in args.image_sizes:\n",
        "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=image_size_, batch_size=1)\n",
        "            print(f'Résolution de {image_size_=}')\n",
        "            # Displays the input image of the model \n",
        "            for i_image, (data, label) in enumerate(dataloaders['test']):                \n",
        "                data, label = data.to(device), label.to(device)\n",
        "\n",
        "                for model_name in models_vgg.keys():\n",
        "                    model = models_vgg[model_name]\n",
        "                    model = model.to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
        "                        tic = time.time()\n",
        "                        out = model(data).squeeze(0)\n",
        "                        _, indices = torch.sort(out, descending=True)\n",
        "                        if model_name == 'vgg' : # our previous work\n",
        "                            top_1 = labels[indices[0]]\n",
        "                            percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
        "                            perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
        "                        else :\n",
        "                            top_1 = subset_labels[indices[0]] \n",
        "                            percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
        "                            perf_ = percentage[label].item()\n",
        "                        dt = time.time() - tic\n",
        "                    print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds, best confidence for : {top_1}')\n",
        "                    df_downsample.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
        "                                       'label':labels[i_label_top], 'i_label':i_label_top, \n",
        "                                       'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'image_size': image_size_, 'device_type':device.type, 'top_1':str(top_1)}\n",
        "                    i_trial += 1\n",
        "\n",
        "            df_downsample.to_json(filename)\n",
        "\n",
        "main()            "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, again, we collect our results, and display all the data in a table "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'results/{datetag}_results_2_{args.HOST}.json'\n",
        "#filename = 'results/2021-04-20_results_2_INV-133-DE01.json'\n",
        "df_downsample = pd.read_json(filename)\n",
        "df_downsample"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy\n",
        "\n",
        "And extract the accuracy, the precision ans the recall for each networks :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for image_size in args.image_sizes:\n",
        "    pprint(f'Benchmarking image size = {image_size}')\n",
        "    df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"top_1\"], \n",
        "                                                                     df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"label\"])\n",
        "                                        for subset_label in subset_labels} \n",
        "                           for model_name in models_vgg.keys()})\n",
        "\n",
        "    ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "    plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "    ax.set_title(f'Experiment 2 - image size = {image_size}', size=20)\n",
        "    ax.set_ylabel('Accuracy top_1', size=14)\n",
        "    plt.show();\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.DataFrame({model_name: {image_size: accuracy_score(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"top_1\"], \n",
        "                                                               df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"label\"])\n",
        "                                    for image_size in args.image_sizes} \n",
        "                       for model_name in models_vgg.keys()})\n",
        "\n",
        "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
        "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "plt.show();"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
        "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "plt.show();"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation time\n",
        "\n",
        "A display of the differents computation time of each models on the same dataset for differents resolutions :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "for color, model_name in zip(colors, models_vgg.keys()):\n",
        "    axs = sns.violinplot(x=\"image_size\", y=\"time\", data=df_downsample, inner=\"quartile\", hue='model')\n",
        "    axs.set_title('Processed on : ' + args.HOST + '_' + str(df_downsample['device_type'][0]), size = 20)\n",
        "    axs.set_ylabel('Computation time (s)', size=18)\n",
        "    axs.set_xlabel('Trial', size=18)\n",
        "    axs.set_yscale('log')\n",
        "h, l = axs.get_legend_handles_labels()\n",
        "axs.legend(h[:5], l[:5], loc='upper center', fontsize=16);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification likelihood\n",
        "\n",
        "Let's display the likelihood of each models on the same dataset for differents resolutions. Here accuracies are displayed as a violin plot to allow a better representation of the models."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "axs = sns.violinplot(x=\"image_size\", y=\"perf\", data=df_downsample, inner=\"quartile\", hue='model', cut = 0, scale = 'width')\n",
        "axs.set_title('Processed on : ' + args.HOST + '_' + str(df_downsample['device_type'][0]), size=20)\n",
        "axs.set_ylabel('Classification likelihood (%)', size=18)\n",
        "axs.set_xlabel('Image size', size=18)\n",
        "h, l = axs.get_legend_handles_labels()\n",
        "axs.legend(h[:5], l[:5], loc ='center', fontsize=16);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image display\n",
        "\n",
        "The 64 worsts classification likelihood, all models and sizes combined : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N_image_i = 8\n",
        "N_image_j = 8\n",
        "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
        "for i, idx in enumerate(df_downsample[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
        "    ax = axs[i%N_image_i][i//N_image_i]\n",
        "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df_downsample.loc[idx]['i_image']][0]))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlabel(df_downsample.loc[idx]['label'] + ' | ' + df_downsample.loc[idx]['model']+ ' | ' + str(df_downsample.loc[idx]['image_size']), color='g')\n",
        "    perf_ = df_downsample.loc[idx]['perf']\n",
        "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summary\n",
        "\n",
        "Again, we extracted a specific mean for each models : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean accuracy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    pprint(f'Benchmarking model {model_name}')\n",
        "    for image_size in args.image_sizes:\n",
        "        df_ = df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)]\n",
        "        mean_acc = (df_[\"top_1\"] == df_[\"label\"]).mean()\n",
        "        print(f'For size {image_size}, the mean accuracy = {mean_acc*100:.4f} %' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification likelihood's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    pprint(f'Benchmarking model {model_name}')\n",
        "    for image_size in args.image_sizes:\n",
        "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"perf\"])\n",
        "        print(f'For size {image_size}, the mean clasification likelihood = {med_perf:.5f} %' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation time 's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    pprint(f'Benchmarking model {model_name}')\n",
        "    for image_size in args.image_sizes:\n",
        "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"time\"])\n",
        "        print(f'For size {image_size}, the mean computation time = {med_perf:.3f} s' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frame per second's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    pprint(f'Benchmarking model {model_name}')\n",
        "    for image_size in args.image_sizes:\n",
        "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"fps\"])\n",
        "        print(f'For size {image_size}, the mean fps = {med_perf:.3f} Hz' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Image processing and recognition on grayscale images :\n",
        "\n",
        "Again, same likelihood indicators but now with a grayscale transformation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'experiment_grayscale.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "#import model's script and set the output file\n",
        "from DCNN_transfer_learning.model import *\n",
        "filename = f'results/{datetag}_results_3_{args.HOST}.json'\n",
        "\n",
        "def main():\n",
        "    if os.path.isfile(filename):\n",
        "        df_gray = pd.read_json(filename)\n",
        "    else:\n",
        "        i_trial = 0\n",
        "        df_gray = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device_type', 'top_1']) \n",
        "        # image preprocessing setting a grayscale output\n",
        "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=1, batch_size=1)\n",
        "\n",
        "        # Displays the input image of the model \n",
        "        for i_image, (data, label) in enumerate(dataloaders['test']):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "\n",
        "            for model_name in models_vgg.keys():\n",
        "                model = models_vgg[model_name]\n",
        "                model = model.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
        "                    tic = time.time()\n",
        "                    out = model(data).squeeze(0)\n",
        "                    if model_name == 'vgg' :\n",
        "                        percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
        "                        _, indices = torch.sort(out, descending=True)\n",
        "                        top_1 = labels[indices[0]]\n",
        "                        perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
        "                    else :\n",
        "                        percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
        "                        _, indices = torch.sort(out, descending=True)\n",
        "                        top_1 = subset_labels[indices[0]] \n",
        "                        perf_ = percentage[label].item()\n",
        "                dt = time.time() - tic\n",
        "                df_gray.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
        "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
        "                                   'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'device_type':device.type, 'top_1':str(top_1)}\n",
        "                print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds, best confidence for : {top_1}')\n",
        "                i_trial += 1\n",
        "        df_gray.to_json(filename)\n",
        "\n",
        "main()    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Collecting all the results, displaying all the data in a table "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f'results/{datetag}_results_3_{args.HOST}.json'\n",
        "#filename = 'results/2021-04-20_results_3_INV-133-DE01.json'\n",
        "df_gray = pd.read_json(filename)\n",
        "df_gray"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(df_gray['model'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image display\n",
        "\n",
        "The 64 worsts classification likelihood, all model combined : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "N_image_i = 8\n",
        "N_image_j = 8\n",
        "\n",
        "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
        "for i, idx in enumerate(df_gray[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
        "    ax = axs[i%N_image_i][i//N_image_i]\n",
        "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df_gray.loc[idx]['i_image']][0], pilmode=\"L\"), cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlabel(df_gray.loc[idx]['label'] + ' | ' + df_gray.loc[idx]['model'], color='g')\n",
        "    perf_ = df_gray.loc[idx]['perf']\n",
        "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification likelihood compared with experiment 1\n",
        "\n",
        "Let's analyze the classification likelihood of each models on the same dataset for color versus grayscale images. Here likelihood's are displayed as a violin plot to allow a better representation of the models."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi**2))\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "for color, df_, label in zip(['gray', 'red'], [df_gray, df], ['black', 'color']):\n",
        "    axs = sns.violinplot(x=\"model\", y=\"perf\", data=df_, inner=\"quartile\", cut=0, color=color, alpha=.5, scale = 'width')\n",
        "axs.set_title('Processed on : ' + args.HOST + '_' + str(df_['device_type'][0]), size=20)\n",
        "axs.set_ylabel('Classification likelihood (%)', size=18)\n",
        "axs.set_xlabel('Model', size=18)\n",
        "axs.legend(['Color', 'Gray'], fontsize=18)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification likelihood's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf_orig = np.mean(df[df['model']==model_name][\"perf\"])\n",
        "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"perf\"])\n",
        "    print(f'For the {model_name} model, the mean clasification likelihood = {med_perf:.5f} % (color = {med_perf_orig:.5f} % )' )\n",
        "    print(stats.ttest_1samp(df_gray[df_gray['model']==model_name][\"perf\"], np.mean(df[df['model']==model_name][\"perf\"])))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy of the learned models on grayscale images"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "results = {} \n",
        "results['acc'] = []\n",
        "results['precision'] = {}\n",
        "results['recall'] = {}\n",
        "\n",
        "for model_name in models_vgg.keys():\n",
        "    #pprint('Model : ' + model_name)\n",
        "    model_pred = df_gray[df_gray['model']==model_name][\"top_1\"].values.tolist()\n",
        "    ground_truth = df_gray[df_gray['model']==model_name][\"label\"].values.tolist()\n",
        "    acc = sklearn.metrics.accuracy_score(ground_truth, model_pred)\n",
        "    results['acc'].append(acc)\n",
        "    \n",
        "    #print(f'Accuracy top_1 : {acc:.3f}')\n",
        "    pres = sklearn.metrics.multilabel_confusion_matrix(ground_truth, model_pred)\n",
        "    results['precision'][model_name] = []\n",
        "    #pprint('Precision :')\n",
        "    for i, label in zip(pres, subset_labels) :\n",
        "        pres_ = np.flip(i)\n",
        "        pres_ = ((pres_[0][0]) / (pres_[0][0] + pres_[-1][0]))\n",
        "        results['precision'][model_name].append(pres_)\n",
        "        #print(f'{pres_:.3f} for the label {label}')\n",
        "    results['recall'][model_name] = []\n",
        "    #pprint('Recall :')\n",
        "    for i, label in zip(pres, subset_labels) :\n",
        "        pres_ = np.flip(i)\n",
        "        pres_ = ((pres_[0][0]) / (pres_[0][0] + pres_[0][-1]))\n",
        "        results['recall'][model_name].append(pres_)\n",
        "        #print(f'{pres_:.3f} for the label {label}')\n",
        "\n",
        "for mode in ['recall', 'precision']:\n",
        "    df_acc = pd.DataFrame({'vgg16_gray': results[mode]['vgg16_gray'],\n",
        "                        'vgg16_lin': results[mode]['vgg16_lin'],\n",
        "                         'vgg16_gen': results[mode]['vgg16_gen'],\n",
        "                          'vgg16_scale': results[mode]['vgg16_scale'],\n",
        "                           'vgg': results[mode]['vgg'],\n",
        "                      }, index=subset_labels)\n",
        "    ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize = 10)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "    ax.set_title(mode.capitalize()+' for each models - experiment 3', size=20)\n",
        "    ax.set_ylabel(mode.capitalize(), size=14)\n",
        "    ax.set_xlabel('Labels', size=16)\n",
        "    \n",
        "df_acc = pd.DataFrame({'accuracy': results['acc']},\n",
        "                  index = models_vgg.keys())\n",
        "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize = 18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
        "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=14, fmt='%.3f')\n",
        "ax.set_title('Accuracy top_1 : for each models - experiment 3', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_xlabel('Models', size=14)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean accuracy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    mean_acc_orig = (df[df['model']==model_name][\"top_1\"] == df[df['model']==model_name][\"label\"]).mean()\n",
        "    mean_acc = (df_gray[df_gray['model']==model_name][\"top_1\"] == df_gray[df_gray['model']==model_name][\"label\"]).mean()\n",
        "    print(f'For the {model_name} model, the mean clasification likelihood = {mean_acc*100:.5f} % (color = {mean_acc_orig*100:.5f} % )' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
        "                                                                 df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"])\n",
        "                                    for subset_label in subset_labels} \n",
        "                       for model_name in models_vgg.keys()})\n",
        "\n",
        "ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title('Accuracy top_1 : for each models - experiment 1', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "ax.set_xlabel('Models', size=14);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.DataFrame({model_name: {label: accuracy_score(df_[(df_['model']==model_name)][\"top_1\"], \n",
        "                                                               df_[(df_['model']==model_name)][\"label\"])\n",
        "                                    for label, df_ in zip(['original', 'gray'], [df, df_gray])} \n",
        "                       for model_name in models_vgg.keys()})\n",
        "\n",
        "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
        "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
        "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
        "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
        "ax.set_title(f'Experiment 3 - color vs gray images', size=20)\n",
        "ax.set_ylabel('Accuracy', size=14)\n",
        "plt.show();"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : analyse *post-hoc* effect of contrast or salience"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation time\n",
        "\n",
        "A display of the differents computation time of each models on the same dataset for a single resolution :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_gray['time'].min(), df_gray['time'].max(), df_gray['time'].quantile(.005), len(df_gray[df_gray['model']==model_name]['time'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation time 's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf_orig = np.mean(df[df['model']==model_name][\"time\"])\n",
        "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"time\"])\n",
        "    print(f'For the {model_name} model, the mean computation time = {med_perf:.4f} s (color = {med_perf_orig:.4f} s )' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frame per second's mean"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models_vgg.keys():\n",
        "    med_perf_orig = np.mean(df[df['model']==model_name][\"fps\"])\n",
        "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"fps\"])\n",
        "    print(f'For the {model_name} model, the mean fps = {med_perf:.3f} Hz (color = {med_perf_orig:.3f} Hz )' )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2))\n",
        "for color, df_, label, legend in zip(['gray', 'red'], [df_gray, df], ['black', 'color'], ['Grayscale', 'Regular']):\n",
        "    for ax, model_name in zip(axs, models_vgg.keys()):\n",
        "        ax.set_ylabel('Frequency', fontsize=14) \n",
        "        df_[df_['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=str(legend+ ' ' + model_name), ax=ax, color=color, density=True)\n",
        "        ax.legend(loc='upper right', fontsize=20)\n",
        "        ax.set_xlim(df_gray['time'].quantile(.01), df_gray['time'].quantile(.99))\n",
        "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
        "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification likelihood\n",
        "\n",
        "A display of the classification likelihood of each models on the same dataset for a single resolution :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
        "# plt.xticks(fontsize=18)\n",
        "# plt.yticks(fontsize=18)\n",
        "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
        "    ax.set_ylabel('Frequency', fontsize=14)\n",
        "    df_gray[df_gray['model']==model_name]['perf'].plot.hist(bins=np.linspace(90, 100, 100), lw=0, alpha=0.3, label=model_name + '_gray', ax=ax, color='k', density=True)\n",
        "    df[df['model']==model_name]['perf'].plot.hist(bins=np.linspace(90, 100, 100), lw=0, alpha=0.3, label=model_name + '_color', ax=ax, color=color, density=True)\n",
        "    ax.legend(loc='upper left', fontsize=20)\n",
        "    ax.set_xlim(90, 100)\n",
        "    ax.set_ylim(0, 1)\n",
        "    # ax.tick_params(axis='x', labelsize=14)\n",
        "    # ax.tick_params(axis='y', labelsize=14)\n",
        "axs[-1].set_xlabel('Classification likelihood (%)', size=18)\n",
        "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summary\n",
        "\n",
        "\n",
        "As a summary, we have shown here that one can implement transfer learning on a subset of images and that it reaches a higher accuracy than a simpler hypothesis. We have also shown that transfer learning could also be used to teach the network to different perturbations of the image, for instance changes in the resolution of in the colorspace. Such framework is thus promising for further applications which aim at modelling higher-order cognitive processes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: scan of some parameters\n",
        "\n",
        "If there is some GPU time time left, let's try to *meta*-optimize some parameters by testing how accuracy would vary. To avoid potential over-fitting problems, we perform that test on the `val` validation set which is separate from the `test` and `train` datasets. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scriptname = 'experiment_scan.py'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {scriptname}\n",
        "\n",
        "#import model's script and set the output file\n",
        "from DCNN_transfer_learning.model import *\n",
        "from experiment_train import train_model\n",
        "\n",
        "scan_dicts= {'batch_size' : [8, 13, 21, 34, 55],\n",
        "             'lr': args.lr * np.logspace(-1, 1, 7, base=10),\n",
        "             'momentum': 1 - np.logspace(-3, -.5, 7, base=10),\n",
        "             'beta2': 1 - np.logspace(-5, -1, 7, base=10),\n",
        "            }\n",
        "\n",
        "def main(N_avg=10):\n",
        "\n",
        "    for key in scan_dicts:\n",
        "        filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
        "        print(f'{filename=}')\n",
        "        if os.path.isfile(filename):\n",
        "            df_scan = pd.read_json(filename)\n",
        "        else:\n",
        "            i_trial = 0\n",
        "            measure_columns = [key, 'avg_loss_val', 'avg_acc_val', 'time']\n",
        "\n",
        "            df_scan = pd.DataFrame([], columns=measure_columns) \n",
        "            for i_trial, value in enumerate(scan_dicts[key]):\n",
        "                new_kwarg = {key: value}\n",
        "                print('trial', i_trial, ' /', len(scan_dicts[key]))\n",
        "                print('new_kwarg', new_kwarg)\n",
        "                # Training and saving the network\n",
        "                models_vgg_ = torchvision.models.vgg16(pretrained=True)\n",
        "                # Freeze training for all layers\n",
        "                # Newly created modules have require_grad=True by default\n",
        "                for param in models_vgg_.features.parameters():\n",
        "                    param.require_grad = False \n",
        "\n",
        "                num_features = models_vgg_.classifier[-1].in_features\n",
        "                features = list(models_vgg_.classifier.children())[:-1] # Remove last layer\n",
        "                features.extend([nn.Linear(num_features, n_output)]) # Add our layer with `n_output` outputs\n",
        "                models_vgg_.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "                since = time.time()\n",
        "\n",
        "                (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=0, **new_kwarg)\n",
        "                models_vgg_, df_train = train_model(models_vgg_, num_epochs=args.num_epochs//4, dataloaders=dataloaders, **new_kwarg)\n",
        "\n",
        "                elapsed_time = time.time() - since\n",
        "                print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
        "\n",
        "                df_scan.loc[i_trial] = {key:value, 'avg_loss_val':df_train.iloc[-N_avg:-1]['avg_loss_val'].mean(), \n",
        "                                   'avg_acc_val':df_train.iloc[-N_avg:-1]['avg_acc_val'].mean(), 'time':elapsed_time}\n",
        "                print(df_scan.loc[i_trial])\n",
        "                i_trial += 1\n",
        "            df_scan.to_json(filename)\n",
        "\n",
        "main()    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -int {scriptname}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for key in scan_dicts:\n",
        "    filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
        "    df_scan = pd.read_json(filename)\n",
        "    print(df_scan)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scan.plot()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_scan)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "toc-autonumbering": true,
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 4
}