{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing transfer learning on Vgg16 using pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! I am  [Jean-Nicolas Jérémie](https://laurentperrinet.github.io/author/jean-nicolas-jeremie/) and the goal of this notebook is to provide a framework to implement (and experiment with) [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) on deep convolutional neuronal network (DCNN). In a nutshell, [transfer learning](https://www.analyticsvidhya.com/blog/2021/06/transfer-learning-using-vgg16-in-pytorch/) allows to re-use the knowlegde learned on a problem, such as categorizing images from  a large dataset, and apply it to a different (yet related) problem, performing the categorization on a smaller dataset. It is a powerful method as it allows to implement complex task *de novo* quite rapidly (in a few hours) without having to retrain the millions of parameters of a DCNN (which takes days of computations). The basic hypothesis is that it suffices to [re-train the last classification layers](https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch) (the head) while keeping the first layers fixed. Here, these networks teach us also some interesting insights into how living systems may perform such categorization tasks.\n",
    "\n",
    "Based on our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html), we will start from a [VGG16 network](https://pytorch.org/hub/pytorch_vision_vgg/) loaded from the [`torchvision.models` library](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py) and pre-trained on the [Imagenet](http://image-net.org/) dataset wich allows to perform label detection on naturals images for $K = 1000$ labels. Our goal here will be to re-train the last fully-Connected layer of the network to perfom the same task but in a sub-set of $K = 10$ labels from the Imagenet dataset. \n",
    "\n",
    "Moreover, we are going to evaluate different strategies of transfer learning:\n",
    "\n",
    "* VGG General : Substitute the last layer of the pyTorch VGG16 network ($K = 1000$ labels) with a new layer build from a specific subset ($K = 10$ labels).\n",
    "* VGG Linear : Add a new layer build from a specific subset ($K = 10$ labels) after the last Fully-Connected layer of the the pyTorch VGG16 network.\n",
    "* VGG Gray : Same architecture as the VGG General network but trained with grayscale images.\n",
    "* VGG Scale : Same architecture as the VGG General network but trained with images of different size.\n",
    "\n",
    "In this notebook, I will use the [pyTorch](https://pytorch.org/) library for running the networks and the [pandas](https://pandas.pydata.org/docs/getting_started/index.html) library to collect and display the results. This notebook was done during a master 2 internship at the Neurosciences Institute of Timone (INT) under the supervision of [Laurent PERRINET](https://laurentperrinet.github.io/). It is curated in the following [github repo](https://github.com/JNJER/2020-06-26_fast_and_curious.git).\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "In our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html), as the VGG16 network was first trained on the entire dataset of $K=1000$ labels, and in order to recover the classification confidence predicted by the model according to the specific subset of classes ($K = 10$ labels) on which it is tested, the output `softmax` mathematical function of the last layer of the network was slightly changed. By assuming that we know *a priori* that the image belongs to one (and only one) category from the sub-set the probabilities obtained would correspond to a confidence of classification discriminating only the classes of interest and can be compared to a chance level of $1 /K$. This creates another network (which is not retrained) directly based on VGG:\n",
    "\n",
    "* VGG Subset : Just consider the specific subset ($K = 10$ labels) from the last layer of the pyTorch VGG16 network ($K = 1000$ labels).\n",
    "\n",
    "This notebook aims in addition to test this hypothesis. Our use case consists of measuring whether there are differences in the likelihood of these networks during an image recognition task on a sub-set of $1000$ classes of the `ImageNet` library, with $K = 10$ (experiment 1). Additionally, we will implement some image transformations as up/down-sampling (experiment 2) or transforming to grayscale (experiment 3) to quantify their influence on the accuracy and computation time of each network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Some useful links :*\n",
    "\n",
    "* https://jaketae.github.io/study/pytorch-vgg/\n",
    "* https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
    "* https://www.kaggle.com/paultimothymooney/detect-retina-damage-from-oct-images/notebook\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Transfer_learning\n",
    "* https://github.com/laurentperrinet/ImageNet-Datasets-Downloader/tree/8d1c0925b5512f48978177a76e7b851ff40acb7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (21.3.1)\n",
      "Requirement already satisfied: matplotlib in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.4.3)\n",
      "Requirement already satisfied: numpy in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.21.3)\n",
      "Requirement already satisfied: imageio in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: torch in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.11.1)\n",
      "Requirement already satisfied: pandas in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.3.4)\n",
      "Requirement already satisfied: requests in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.26.0)\n",
      "Requirement already satisfied: sklearn in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.0)\n",
      "Requirement already satisfied: scipy in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: seaborn in /home/laurent/.local/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (0.11.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 5)) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/laurent/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/laurent/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/laurent/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions in /home/laurent/.local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 8)) (3.10.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 10)) (2019.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->-r requirements.txt (line 11)) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/laurent/.local/lib/python3.8/site-packages (from requests->-r requirements.txt (line 11)) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/laurent/.local/lib/python3.8/site-packages (from requests->-r requirements.txt (line 11)) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->-r requirements.txt (line 11)) (2.8)\n",
      "Requirement already satisfied: scikit-learn in /home/laurent/.local/lib/python3.8/site-packages (from sklearn->-r requirements.txt (line 12)) (0.23.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/laurent/.local/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/laurent/.local/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 12)) (0.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!make clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 93720\n",
      "-rw-r--r-- 1 501 dialout 95551495 Apr  2  2021 Imagenet_urls_ILSVRC_2016.json\n",
      "-rw-r--r-- 1 501 dialout      886 Oct  5 14:47 README.md\n",
      "-rw-r--r-- 1 501 dialout   121566 Oct  5 14:47 imagenet_label_to_wordnet_synset.json\n",
      "drwxr-xr-x 1 501 dialout      256 Oct  7 15:17 \u001b[0m\u001b[01;34mfigures\u001b[0m/\n",
      "-rw-r--r-- 1 501 dialout      391 Oct 25 16:30 Makefile\n",
      "drwxr-xr-x 1 501 dialout       64 Oct 25 23:51 \u001b[01;34mmodels\u001b[0m/\n",
      "drwxr-xr-x 1 501 dialout      192 Oct 26 08:00 \u001b[01;34mDCNN_transfer_learning\u001b[0m/\n",
      "-rw-r--r-- 1 501 dialout      221 Oct 26 09:25 requirements.txt\n",
      "-rw-r--r-- 1 501 dialout     5798 Oct 26 09:40 experiment_train.py\n",
      "-rw-r--r-- 1 501 dialout     2362 Oct 26 09:40 experiment_basic.py\n",
      "-rw-r--r-- 1 501 dialout     2609 Oct 26 09:40 experiment_downsample.py\n",
      "-rw-r--r-- 1 501 dialout     2402 Oct 26 09:40 experiment_grayscale.py\n",
      "-rw-r--r-- 1 501 dialout     2504 Oct 26 09:40 experiment_scan.py\n",
      "drwxr-xr-x 1 501 dialout       96 Oct 26 09:40 \u001b[01;34m__pycache__\u001b[0m/\n",
      "drwxr-xr-x 1 501 dialout     7296 Oct 26 13:23 \u001b[01;34mresults\u001b[0m/\n",
      "drwxr-xr-x 1 501 dialout      192 Oct 26 13:26 \u001b[01;34mdata\u001b[0m/\n",
      "-rw-r--r-- 1 501 dialout   224323 Oct 26 13:36 benchmark_transfer_learning_VGG.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# uncommment to re-run training\n",
    "#%rm -fr models\n",
    "%mkdir -p DCNN_transfer_learning\n",
    "%mkdir -p results\n",
    "%mkdir -p models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the libraries/variables\n",
    "\n",
    "Our coding strategy is to build up a small library as a package of scripts in the `DCNN_benchmark` folder and to run all calls to that library from this notebook. This follows our [previous work]( https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html) in which we benchmarked various DCNNs and which allowed us to select VGG16 as a good compromise between performance and complexity.\n",
    "\n",
    "First of all, a `init.py` script defines all our usefull variables like the new labels to learn, the number of training images or the root folder to use. Also, we import libraries to train the different networks and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_transfer_learning/init.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_transfer_learning/init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "# Importing libraries\n",
    "import torch\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=18)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=18)    # fontsize of the tick labels\n",
    "import numpy as np\n",
    "#from numpy import random\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from time import strftime, gmtime\n",
    "datetag = strftime(\"%Y-%m-%d\", gmtime())\n",
    "datetag = '2021-10-26'\n",
    "\n",
    "HOST, device = os.uname()[1], torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HOST, device = 'inv-ope-de06', torch.device(\"cuda\")\n",
    "\n",
    "    \n",
    "# to store results\n",
    "import pandas as pd\n",
    "\n",
    "def arg_parse():\n",
    "    DEBUG = 25\n",
    "    DEBUG = 1\n",
    "    parser = argparse.ArgumentParser(description='DCNN_transfer_learning/init.py set root')\n",
    "    parser.add_argument(\"--root\", dest = 'root', help = \"Directory containing images to perform the training\",\n",
    "                        default = 'data', type = str)\n",
    "    parser.add_argument(\"--folders\", dest = 'folders', help =  \"Set the training, validation and testing folders relative to the root\",\n",
    "                        default = ['test', 'val', 'train'], type = list)\n",
    "    parser.add_argument(\"--N_images\", dest = 'N_images', help =\"Set the number of images per classe in the train folder\",\n",
    "                        default = [400//DEBUG, 200//DEBUG, 800//DEBUG], type = list)\n",
    "    parser.add_argument(\"--HOST\", dest = 'HOST', help = \"Set the name of your machine\",\n",
    "                    default=HOST, type = str)\n",
    "    parser.add_argument(\"--datetag\", dest = 'datetag', help = \"Set the datetag of the result's file\",\n",
    "                    default = datetag, type = str)\n",
    "    parser.add_argument(\"--image_size\", dest = 'image_size', help = \"Set the default image_size of the input\",\n",
    "                    default = 256)\n",
    "    parser.add_argument(\"--image_sizes\", dest = 'image_sizes', help = \"Set the image_sizes of the input for experiment 2 (downscaling)\",\n",
    "                    default = [64, 128, 256, 512], type = list)\n",
    "    parser.add_argument(\"--num_epochs\", dest = 'num_epochs', help = \"Set the number of epoch to perform during the traitransportationning phase\",\n",
    "                    default = 200//DEBUG)\n",
    "    parser.add_argument(\"--batch_size\", dest = 'batch_size', help=\"Set the batch size\", default = 32)\n",
    "    parser.add_argument(\"--lr\", dest = 'lr', help=\"Set the learning rate\", default = 0.0001)\n",
    "    parser.add_argument(\"--momentum\", dest = 'momentum', help=\"Set the momentum\", default = 0.9)\n",
    "    parser.add_argument(\"--beta2\", dest = 'beta2', help=\"Set the second momentum - use zero for SGD\", default = 0.)\n",
    "    parser.add_argument(\"--subset_i_labels\", dest = 'subset_i_labels', help=\"Set the labels of the classes (list of int)\",\n",
    "                    default = [945, 513, 886, 508, 786, 310, 373, 145, 146, 396], type = list)\n",
    "    parser.add_argument(\"--class_loader\", dest = 'class_loader', help = \"Set the Directory containing imagenet downloaders class\",\n",
    "                        default = 'imagenet_label_to_wordnet_synset.json', type = str)\n",
    "    parser.add_argument(\"--url_loader\", dest = 'url_loader', help = \"Set the file containing imagenet urls\",\n",
    "                        default = 'Imagenet_urls_ILSVRC_2016.json', type = str)\n",
    "    parser.add_argument(\"--model_path\", dest = 'model_path', help = \"Set the path to the pre-trained model\",\n",
    "                        default = 'models/re-trained_', type = str)\n",
    "    parser.add_argument(\"--model_names\", dest = 'model_names', help = \"Modes for the new trained networks\",\n",
    "                        default = ['vgg16_lin', 'vgg16_gen', 'vgg16_scale', 'vgg16_gray', ], type = list)\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = arg_parse()\n",
    "datetag = args.datetag\n",
    "json_fname = os.path.join('results', datetag + '_config_args.json')\n",
    "load_parse = False # False to custom the config\n",
    "\n",
    "if load_parse:\n",
    "    with open(json_fname, 'rt') as f:\n",
    "        print(f'file {json_fname} exists: LOADING')\n",
    "        override = json.load(f)\n",
    "        args.__dict__.update(override)\n",
    "else:\n",
    "    print(f'Creating file {json_fname}')\n",
    "    with open(json_fname, 'wt') as f:\n",
    "        json.dump(vars(args), f, indent=4)\n",
    "    \n",
    "# matplotlib parameters\n",
    "colors = ['b', 'r', 'k', 'g', 'm']\n",
    "fig_width = 20\n",
    "phi = (np.sqrt(5)+1)/2 # golden ratio for the figures :-)\n",
    "\n",
    "#to plot & display \n",
    "def pprint(message): #display function\n",
    "    print('-'*len(message))\n",
    "    print(message)\n",
    "    print('-'*len(message))\n",
    "    \n",
    "#DCCN training\n",
    "print('On date', args.datetag, ', Running benchmark on host', args.HOST, ' with device', device.type)\n",
    "\n",
    "# Labels Configuration\n",
    "N_labels = len(args.subset_i_labels)\n",
    "\n",
    "paths = {}\n",
    "N_images_per_class = {}\n",
    "for folder, N_image in zip(args.folders, args.N_images):\n",
    "    paths[folder] = os.path.join(args.root, folder) # data path\n",
    "    N_images_per_class[folder] = N_image\n",
    "    os.makedirs(paths[folder], exist_ok=True)\n",
    "    \n",
    "with open(args.class_loader, 'r') as fp: # get all the classes on the data_downloader\n",
    "    imagenet = json.load(fp)\n",
    "\n",
    "# gathering labels\n",
    "labels = []\n",
    "class_wnids = []\n",
    "reverse_id_labels = {}\n",
    "for a, img_id in enumerate(imagenet):\n",
    "    reverse_id_labels[str('n' + (imagenet[img_id]['id'].replace('-n','')))] = imagenet[img_id]['label'].split(',')[0]\n",
    "    labels.append(imagenet[img_id]['label'].split(',')[0])\n",
    "    if int(img_id) in args.subset_i_labels:\n",
    "        class_wnids.append('n' + (imagenet[img_id]['id'].replace('-n','')))    \n",
    "        \n",
    "# a reverse look-up-table giving the index of a given label (within the whole set of imagenet labels)\n",
    "reverse_labels = {}\n",
    "for i_label, label in enumerate(labels):\n",
    "    reverse_labels[label] = i_label\n",
    "# a reverse look-up-table giving the index of a given i_label (within the sub-set of classes)\n",
    "reverse_subset_i_labels = {}\n",
    "for i_label, label in enumerate(args.subset_i_labels):\n",
    "    reverse_subset_i_labels[label] = i_label\n",
    "    \n",
    "# a reverse look-up-table giving the label of a given index in the last layer of the new model (within the sub-set of classes)\n",
    "subset_labels = []\n",
    "pprint('List of Pre-selected classes : ')\n",
    "# choosing the selected classes for recognition\n",
    "for i_label, id_ in zip(args.subset_i_labels, class_wnids) : \n",
    "    subset_labels.append(labels[i_label])\n",
    "    print('-> label', i_label, '=', labels[i_label], '\\nid wordnet : ', id_)\n",
    "subset_labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file results/2021-10-26_config_args.json\n",
      "On date 2021-10-26 , Running benchmark on host inv-ope-de06  with device cuda\n",
      "-------------------------------\n",
      "List of Pre-selected classes : \n",
      "-------------------------------\n",
      "-> label 945 = bell pepper \n",
      "id wordnet :  n02056570\n",
      "-> label 513 = cornet \n",
      "id wordnet :  n02058221\n",
      "-> label 886 = vending machine \n",
      "id wordnet :  n02219486\n",
      "-> label 508 = computer keyboard \n",
      "id wordnet :  n02487347\n",
      "-> label 786 = sewing machine \n",
      "id wordnet :  n02643566\n",
      "-> label 310 = ant \n",
      "id wordnet :  n03085013\n",
      "-> label 373 = macaque \n",
      "id wordnet :  n03110669\n",
      "-> label 145 = king penguin \n",
      "id wordnet :  n04179913\n",
      "-> label 146 = albatross \n",
      "id wordnet :  n04525305\n",
      "-> label 396 = lionfish \n",
      "id wordnet :  n07720875\n",
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.71 s.\n",
      "  System :       0.10 s.\n",
      "Wall time:       0.90 s.\n"
     ]
    }
   ],
   "source": [
    "%run -int {scriptname} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the `train` & `val` dataset\n",
    "\n",
    "In the `dataset.py`, we use an archive of the Imagenet fall 2011 urls to populate datasets based on the pre-selected classes listed in the `DCNN_benchmark/init.py` file. The following script is inspired by [previous work](https://github.com/laurentperrinet/ImageNet-Datasets-Downloader/) in our group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test': 'data/test', 'val': 'data/val', 'train': 'data/train'},\n",
       " ['test', 'val', 'train'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths, args.folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "imread(uri, format=None, **kwargs)\n",
       "\n",
       "Reads an image from the specified file. Returns a numpy array, which\n",
       "comes with a dict of meta data at its 'meta' attribute.\n",
       "\n",
       "Note that the image data is returned as-is, and may not always have\n",
       "a dtype of uint8 (and thus may differ from what e.g. PIL returns).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "uri : {str, pathlib.Path, bytes, file}\n",
       "    The resource to load the image from, e.g. a filename, pathlib.Path,\n",
       "    http address or file object, see the docs for more info.\n",
       "format : str\n",
       "    The format to use to read the file. By default imageio selects\n",
       "    the appropriate for you based on the filename and its contents.\n",
       "kwargs : ...\n",
       "    Further keyword arguments are passed to the reader. See :func:`.help`\n",
       "    to see what arguments are available for a particular format.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/imageio/core/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imageio\n",
    "imageio.imread?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 800, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageio.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Crozet_-_Manchots.jpg/800px-Crozet_-_Manchots.jpg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_transfer_learning/dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_transfer_learning/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_transfer_learning.init import *  \n",
    "verbose = False\n",
    "\n",
    "with open(args.url_loader) as json_file:\n",
    "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)\n",
    "\n",
    "def clean_list(list_dir, patterns=['.DS_Store']):\n",
    "    for pattern in patterns:\n",
    "        if pattern in list_dir: list_dir.remove('.DS_Store')\n",
    "    return list_dir\n",
    "\n",
    "import imageio\n",
    "def get_image(img_url, timeout=3., min_content=5000, verbose=verbose):\n",
    "    if verbose:\n",
    "        print(f'Processing {img_url}')\n",
    "    try:\n",
    "        img_resp = imageio.imread(img_url)\n",
    "        if verbose : print(f\"Success with url {img_url}\")\n",
    "        #img_resp = requests.get(img_url, timeout=timeout)\n",
    "        #if not 'content-type' in img_resp.headers :\n",
    "        #    if verbose : print('No content-type')\n",
    "        #    return False # did not work\n",
    "        #elif not 'image' in img_resp.headers['content-type'] :\n",
    "        #    if verbose : print('Not an image')\n",
    "        #    return False # did not work\n",
    "        #elif (len(img_resp.content) < min_content) :\n",
    "        #    if verbose : print('Content to short')\n",
    "        #    return False # did not work\n",
    "        #else:\n",
    "        #    return img_resp.content # worked!\n",
    "        return img_resp\n",
    "    except Exception as e:\n",
    "        if verbose : print(f\"Failed with {e} for url {img_url}\")\n",
    "        return False # did not work\n",
    "\n",
    "import hashlib # jah.\n",
    "# root folder\n",
    "os.makedirs(args.root, exist_ok=True)\n",
    "# train, val and test folders\n",
    "for folder in args.folders : \n",
    "    os.makedirs(paths[folder], exist_ok=True)\n",
    "    \n",
    "list_urls = {}\n",
    "list_img_name_used = {}\n",
    "for class_wnid in class_wnids:\n",
    "    list_urls[class_wnid] =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
    "    np.random.shuffle(list_urls[class_wnid])\n",
    "    list_img_name_used[class_wnid] = []\n",
    "\n",
    "    # a folder per class in each train, val and test folder\n",
    "    for folder in args.folders : \n",
    "        class_name = reverse_id_labels[class_wnid]\n",
    "        class_folder = os.path.join(paths[folder], class_name)\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "        list_img_name_used[class_wnid] += clean_list(os.listdir(class_folder)) # join two lists\n",
    "    \n",
    "# train, val and test folders\n",
    "for folder in args.folders : \n",
    "    print(f'Folder \\\"{folder}\\\"')\n",
    "\n",
    "    filename = f'results/{datetag}_dataset_{folder}_{args.HOST}.json'\n",
    "    columns = ['img_url', 'img_name', 'is_flickr', 'dt', 'worked', 'class_wnid', 'class_name']\n",
    "    if os.path.isfile(filename):\n",
    "        df_dataset = pd.read_json(filename)\n",
    "    else:\n",
    "        df_dataset = pd.DataFrame([], columns=columns)\n",
    "\n",
    "    for class_wnid in class_wnids:\n",
    "        class_name = reverse_id_labels[class_wnid]\n",
    "        print(f'Scraping images for class \\\"{class_name}\\\"')\n",
    "        class_folder = os.path.join(paths[folder], class_name)\n",
    "        while (len(clean_list(os.listdir(class_folder))) < N_images_per_class[folder]) and (len(list_urls[class_wnid]) > 0):\n",
    "\n",
    "            # pick and remove element from shuffled list \n",
    "            img_url = list_urls[class_wnid].pop()\n",
    "            \n",
    "            if len(df_dataset[df_dataset['img_url']==img_url])==0 : # we have not yet tested this URL yet\n",
    "\n",
    "                # Transform URL into filename\n",
    "                # https://laurentperrinet.github.io/sciblog/posts/2018-06-13-generating-an-unique-seed-for-a-given-filename.html\n",
    "                img_name = hashlib.sha224(img_url.encode('utf-8')).hexdigest() + '.png'\n",
    "\n",
    "                if img_url.split('.')[-1] in ['jpe', 'gif']:\n",
    "                    if verbose: print('Bad extension for the img_url', img_url)\n",
    "                # make sure it was not used in other folders\n",
    "                elif not (img_name in list_img_name_used[class_wnid]):\n",
    "                    tic = time.time()\n",
    "                    img_content = get_image(img_url, verbose=verbose)\n",
    "                    dt = time.time() - tic\n",
    "                    \n",
    "                    worked = img_content is not False\n",
    "                    if worked:\n",
    "                        if verbose : print('Good URl, now saving', img_url, ' in', class_folder, ' as', img_name)\n",
    "                        imageio.imsave(os.path.join(class_folder, img_name), img_content, format='png')\n",
    "                        list_img_name_used[class_wnid].append(img_name)\n",
    "                    df_dataset.loc[len(df_dataset.index)] = {'img_url':img_url, 'img_name':img_name, 'is_flickr':1 if 'flickr' in img_url else 0, 'dt':dt,\n",
    "                                'worked':worked, 'class_wnid':class_wnid, 'class_name':class_name}\n",
    "\n",
    "                print(f'\\r{len(clean_list(os.listdir(class_folder)))} / {N_images_per_class[folder]}', end='' if verbose else '\\n', flush=not verbose)\n",
    "            #print('\\n')\n",
    "        if (len(clean_list(os.listdir(class_folder))) < N_images_per_class[folder]) and (len(list_urls[class_wnid]) == 0): \n",
    "            print('Not enough working url to complete the dataset') \n",
    "    \n",
    "    df_dataset.to_json(filename)\n",
    "\n",
    "\n",
    "if False: \n",
    "    \n",
    "    # replace the file with that URLs that worked - removes the ones that failed\n",
    "    print(f'Replacing file {args.url_loader}')\n",
    "    with open(args.url_loader, 'wt') as f:\n",
    "        json.dump(Imagenet_urls_ILSVRC_2016, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7a1a3915115df9441b0452e809deff8cc1bd01ac8ec3544a04400a13.png']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_img_name_used[class_wnid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7a1a3915115df9441b0452e809deff8cc1bd01ac8ec3544a04400a13.png',\n",
       " '0ce99fffdd5f013fb20735881080854760fde02b183c9d62594801fb.png',\n",
       " '790d7eefdea576b558ee141bc4c254f27d60ce0176222c13cda8f9d1.png',\n",
       " '432e29c87ccbef0143216caadb937c77cea125f78675625c95d1294f.png',\n",
       " 'ac41ffb9be88a016aec6fc3157d1181448edb4f518b9e5ff41c09e6b.png',\n",
       " '1a1f987a255562e5d3627c4818d1d0b6919d76d1530b76544231355d.png',\n",
       " '9e85c2b92f70cb9d868120715d5fb6083b2d10e893ff2aa5863a6c2f.png',\n",
       " '83837d04ac69d3d337999b190f3fd96fb3d3b4a07895b59979816bd4.png',\n",
       " '63d6cef2cc752b1cd1485d149c17ed33117e0bc443ac0be4ace23e12.png',\n",
       " '83cc38b4c332fed1c3622aea2878d1b6ced258a797b2cd7f67af16d4.png',\n",
       " '687d317f7ece91eef893efc8bbee8907ce24b4da0cc363457165f655.png',\n",
       " '20e17edda5a853f3a33cef5a4ae778e1071949ecc3de60773e76925f.png',\n",
       " '2a9c2bb96f64bc97600f67ac1bb1026aed3aa8edd715dca5980752e9.png',\n",
       " '47dd62b81330eab0dc45c70f6cbc75a8e048a16bc8e299b53d8d1ed0.png',\n",
       " '6d2401c0e207a9530f0e8321aad51d0c1219b634551feb8316c9266b.png',\n",
       " '606bb26c78e32d3596298b57dd03919bf08bf8f02af48e47bbfca7d4.png',\n",
       " '540a27b43fdf67aea91a1ab7481cd9afc0222d58a0e8cc65f27b9fe6.png',\n",
       " '6237d3d140d5e984faceac847349d655e08ab1e49afbf29b82a39c5f.png',\n",
       " '75892adbbdaa6c8471922fefce4090a5f07dbdde6ec4657378a1f298.png',\n",
       " 'b79ee5e8f3f18a08b6e3d8680d1a4d1d0c64b441a8cc6d680d4ba6c6.png',\n",
       " 'cce476dd9afbccce8086cbc8394e51a3dbd45cbbe1e85c6c3b925c3a.png',\n",
       " '6c266317141cf2dbe26fd7a46c7f67205cd4cb8afe0a006f69dfe81e.png',\n",
       " '735e52742d265a8da1cf60a332a2d902a0ba5cee2b772794ef9b82cb.png',\n",
       " 'd7632be647f91ea1c2b49ee8a9f17cb79f2bea6f751e1d67eb0b6996.png',\n",
       " 'c79d95efad58047bba5254c32cd46c9ca8481c9c6044cb4620223796.png',\n",
       " '60761cfd8979cf1558378112bfcd3435bb2c2dc75548d91005eca576.png',\n",
       " '3b47fd648574e2f50115a4bc35dc978def719dbe06119fa79e264a5c.png',\n",
       " 'c24d3e24be3e7c059379f625f5b53268345c70441eb65e41ecf4ed03.png',\n",
       " 'd7837c6015904e4a854d44407f408bdf3b0ba1707ae8d8a8f37be4b9.png',\n",
       " 'f7a6752a634438e5815baae47e51ca6b69c54c4b2a63728532edd0b4.png',\n",
       " '9430b3b9430fc805aad0b35d8bff6186d90de53954eb22f49bee4729.png',\n",
       " '224a0844ede3491a4a74574f5c298ac007ea2a6db31c3ac35e520408.png',\n",
       " 'e79fdcf2234b159fb8492f50097fcb0ddabef7bf3f05fb7ec7b3aab4.png',\n",
       " '716c8806453e0249d8e97be08ad5a22cc6d321357ed788f932f0d2f8.png',\n",
       " '9ba781b9b1f03ada8f1012b8d5d9a6d764257ccd2fe03a07009a6517.png',\n",
       " '0e09a7692f65c05d073b183c0ee9686a45cbe305c8c075535645f37c.png',\n",
       " '18a038c2ec8e1dbb6c88e50e931831b63d09458e93a021eb67833bf5.png',\n",
       " '29fdeab0d27a6c91b3fd91f49c4898604a4cfba247eb4aa554225e75.png',\n",
       " '92f8314bb62f54ac19798feeab66ed9fcff86c8e81879c9783f10531.png',\n",
       " '4595932645ed0fb2032d7e6adf892856f3528e0aba4871d18604785c.png',\n",
       " '384e8635066780937083e814e56bca6aaedd4015ea944f1f1d45d63f.png',\n",
       " '9195850d73364cd1d7f530c9448bbc02c8c144508608f0a64ee5bd43.png',\n",
       " 'b62620ac6e76d8b6ca3b2e18a61164ac13633aaadc72584f1e9bc8f1.png',\n",
       " '7ebcdc2267d7276a50267797314902c60677df1ae9a49de51345d938.png',\n",
       " 'ea1f07a573a494c16ce95e5a581c529ed6e7f264d78d6274cccde2eb.png',\n",
       " 'a0ee3ee64182912a89adaf4f10e2fd8a63a9f310859b138e98953823.png',\n",
       " '6b712f0c66bb16e694d0ca166d5754870af75b532a45dddbdce58f43.png',\n",
       " '6530a7eabc6922c20a58046851a6069503deecc073f22a034aab77e8.png',\n",
       " '829bc6813ebd4066b036970bd9ea693212eb8a964c6ad2093b24136a.png',\n",
       " '3b53d4634e59d3622680a13e8f74ff3916c2d520f617b35c8e7335da.png',\n",
       " 'cdff03f9f85268c57aa80ccf99c9ca757e2126e86c02e95d5818f832.png',\n",
       " '0399220d980d778e3309a9ba2b11fd58fb74757b45108bb6a7732561.png',\n",
       " 'a0304cc5e93bc1a3393d5ffcfaaeee159e0387226793f26279d707d7.png',\n",
       " '2a001e27a4c7a7bfb143aaff343e90787a888fa2c58bd2373ac6889d.png',\n",
       " '575c92f7961963664d267edbe9e1c634436aca9bb6c8595734b5249e.png',\n",
       " '713faaf327c2abbf9702d4245b29cdcf7128d3c7a631902d21cc9583.png',\n",
       " 'b7a169b41d1592eb22c46dc2f9151b7b3d9e72c8fa14fdeb1472700a.png',\n",
       " 'fd551c78676a0415078d6a16227ddb8697e47cd37d5a74c6f2d34170.png',\n",
       " 'f580c086f58c19a3e964b015341c35dba0e3a44fc5fbfcf65a269c88.png',\n",
       " '8b204ab604b71022b5a3810ca17d3921d32a5f1a064c6c9e8ecd912c.png',\n",
       " '35b5307e92af4dfc0518e27d8b6ee6c355edaa35ad63908872b43d8e.png',\n",
       " '28736e10a18c3524ada943158dfb2869e0c12b26ab961ba6a482b596.png',\n",
       " '9c67ec47dec0d3986f09984d2eec91dfd079bd768b7c579b1a27aa29.png',\n",
       " '52642ea7b43eaaef547afed75e2268746c91c1f708e283db71832757.png',\n",
       " 'c8dfabd4345fadbe3908f0a4679302193b2d552103d289ef0dae001f.png',\n",
       " '57d9643f11a2a29d21af4e8158f4c35c9a5826492e5fff13cef34f6b.png',\n",
       " '09c82cace14618e7e317623e99512fb260d2ad6c422ca791e53a26c2.png',\n",
       " '37209516be01335ab2a8c8959bb6e2c6e8385f031d2186d61471ad78.png',\n",
       " '5f9462143c623b0c5189e0718b1f799820ac54aecc4fde70153a2abc.png',\n",
       " 'e2fbe5e29645ae1943bef6d525d5e711a59184c332dc805c18e17b8d.png',\n",
       " '366392b8ff09a717e3fe1c14bf655ccc84cc01a110d7e7bf427d7b15.png',\n",
       " 'ec46ee31b55999e7eb39f8fcd0ca42fb6f2a90096ecab3aedd441c72.png',\n",
       " 'f0da37acc580bf7b1c06522e27ce13f8be3b804913732257d77b034f.png',\n",
       " 'cf2a3772bf08440163520995009687b2053290ab725d71d5d3f4fbb1.png',\n",
       " '9745bd8816f666181c18155e4af75a12c5c969a111b748a5847b4714.png',\n",
       " 'e4283c903c4bea98d47652971bc804fd514d2da71d086f199e896c6d.png',\n",
       " '64f6d40250b97509a2ae318ef8681db518e571b01dd63d8236d9a255.png',\n",
       " '7f4d14939b31ac69464273477aff65525974790a6afb711d05e5320b.png',\n",
       " 'b12a03f099fb586fa962a33ba538bc5315ca222809686deae01137c8.png',\n",
       " '7ea39cc673bee09ff5a0332e86b5b2a83813d90ef90487fa85026faf.png',\n",
       " 'c78790a50bd263affdc8ccda23df324ddc5c273688508a6ff9d2b0e2.png',\n",
       " '5c624a112eb5a36dd16ac49cf5c6d881c548c8752037da6b1016ca87.png',\n",
       " '4a35ec7fb182c6fb6b22f5d8b5e6b8892b60c422771a8b2ab96a9b3d.png',\n",
       " 'eb1e069f6d514eec7b38ca015fba1495d64096c80875461d2211d61d.png',\n",
       " 'dc79fa6b68dd57ed6dd28f5e303665f5681c75bfea1f6b4f5f2bd5a7.png',\n",
       " 'ce32c7b63c3a9ff8ded2de1cfed81b41f565a875ed0485dfdb4cdaa4.png',\n",
       " 'dde78bdbba436079f5457721270bc5ae14d2322f54f8af6e22df1dae.png',\n",
       " '4fb7318eb359eae0e8db7427b0a30c8d9ff99ac311e99274881bb527.png',\n",
       " 'd7a8e15024c28f78d7c24639e9b9fbfa38351d59c556ef703e640ad6.png',\n",
       " 'eaf5929c27394a7400a16fdf42830243d8bafef246888555be94b814.png',\n",
       " 'a81a612eb6488ec99fb8cc4eb576d45fc943e78bea3710e81154060e.png',\n",
       " 'faaeb675ec18b3db01d9b3dda390090b58c885ae6ea939641496c9dd.png',\n",
       " 'd6c8276babd0ba1adad9cd108d58419c0b7b043b34dac24790556f99.png',\n",
       " '1d6a3f5bc280889e7418160e001e2ef08a8832b18dc5f96a984dcf18.png',\n",
       " 'e8fcfccd8598d4fbf779a21f704112fd115b32f50a912b453d4a18e1.png',\n",
       " '12de786410553a7d669eb37246478a409028769c7f8a801d1cc0dd13.png',\n",
       " 'a101c31d2773993bfce10e818b06e09441a16304c5ebc67b9b3f8f97.png',\n",
       " 'c9d544cbc51db1f8be243db6508acc0a481b2cfe628ed5a542bb17e4.png',\n",
       " '994ed2a1e883dad58ec4c72218fab6d2c9a7a2a040e5a9e7d40da498.png',\n",
       " 'ff41984df5dfc93fb2bc5f031ecaf31e08df7b9912c4be00c4a3cce2.png',\n",
       " '1edc8612353106a15dc5bd4f4e6740b78a350742a14fa8f6e0af2341.png',\n",
       " '5fd0a4ed3cb036c8e3a45f854eacaa83dd4012082d7df1567126d840.png',\n",
       " '44d27b48a1e0cbd98c00d98a71725fd8c3e48a5865355981a975076e.png',\n",
       " '6866a29b691de5e1b546ec4b6d99a13277f24ef41fb641c8f774db22.png',\n",
       " '49a8e8030a94974708dd14759c85e3b541af0b49121dea0f1edea212.png',\n",
       " '5a5c87f4f679fee81f13a5f780d71d6b803df66c24fe89cbd5056a73.png',\n",
       " '29c3eb1e2336a46e0a0f3307a7f4b5d92eabe769ef649e9ae1f94917.png',\n",
       " '0d251d8d2128920e5caeb0093ecb9a2e1766401e8cc932c1c7f90016.png',\n",
       " '8bb975f64700a942f0538b18fb9528438c2b1a95298a767e5c656919.png',\n",
       " '8dde0ed9ba13e931a470a02e8d0ffd80ad786ac982ae72e96577f0a2.png',\n",
       " '0ed04150b6be08133d9d494fa55a2684d02cf69ab476fd63199074da.png',\n",
       " 'cc41183142c1f0dfad3391a19b08b19ca8a59908fc42d974ce9c9f63.png',\n",
       " '051a9611415975d50bd280a7b21cc23bfbb94e00d2342b01d489af78.png',\n",
       " '139cf9e99888104183e14eb8f9a9ac9fbf4d9b9ec9514cabbe9ea29c.png',\n",
       " '2139e3803249ed283294e44257643458e191341946382a613b1ad68e.png',\n",
       " '655daf1de6f2eedc27a9bbf028ac78607182fb34800531b2ea0adf9f.png',\n",
       " 'a751f638269f7f6c08a30d98bdb515de23a837b20f596de979784a93.png',\n",
       " '512a38a189cf206b2f155a61b99589531ca8f7b8b9e3203250c310e1.png',\n",
       " '71fef589220d68e0a0d4e1e64a8ac7d7410d0ab18b8ba691e7a89204.png',\n",
       " '5c9eedc6224a07ead9400ed04011ffad12012182046e29f87984af95.png',\n",
       " 'b5b04a289fb4d7e7ccf4730321bc429be5c6bc782d9292f503f545a0.png',\n",
       " '8780f16e95210956ff376d35a0bd0a5068316e13d49634d71da463c7.png',\n",
       " '3e0919567653495b5456f5ed75b4bd9d8274d114807fc49c761207cc.png',\n",
       " '37601a467866e4225f12f3eba6b2c98ec74f01b6a94df5ac12a7ce52.png',\n",
       " '085630d8c7024e8446f91729f7e71567e883db21e45f48511a053b06.png',\n",
       " '03ae4db7e1ec40ee7493b63d54ee2bfd84662d7ec94a114930c3a549.png',\n",
       " '1b54ea20cb6c087e00e61f26e6698c781aee687895505c4cde1b41ef.png',\n",
       " 'bef75c753a396912cc55b572f473abd1691605d6774f6887256f9a0f.png',\n",
       " '3f30d4cc4486465ba88ef7b9e8fa2ae9feac8864308ade5457d82d20.png',\n",
       " '0ada331e25bba677c430c7742d98251236f864e7b5985b1ad17e75d9.png',\n",
       " 'fd811db580fc273dac928346d4a5d7897dc18738699d3bb29714e4dd.png',\n",
       " 'a7315ac661d14534fae242cb34d00dbc39d4299bfed188da3b1cc830.png',\n",
       " '1460f97094ceb0c26cfe8f35f4b1a629ff764eebc4ce0957fb083e99.png',\n",
       " '8f87f856e4bf8dc90b8bce87bcb25edfd8e235f22feac5959183ce2f.png',\n",
       " '70c6639bcbfbe70cd5e6bfe3f98c213f31e9386707747a68d18078f0.png',\n",
       " 'a1f4bf6f4ef4524b9634c37836d97e95624123a51fa820cde830f3cc.png',\n",
       " '9a97f09c3b3403097c82bfd48852bc85e097a8b00d6293c4235ac3f6.png',\n",
       " '198bf9ce6ce8086f1e87d547d9f2e98d3e133252793c5c774204eafe.png',\n",
       " '2c3b6e1178b3e40fc1e4a2bc741a26ea30741578eee9e9d850d2dcc7.png',\n",
       " '4f5b6b30342fbd54cc52c220198efd6bea23d162f590c04320dbefb2.png',\n",
       " '05234e1af1a66aad8257d7a9e435a61bbb907d8563af3b67ceb81d9e.png',\n",
       " 'd1a2b01bb8233d8ea3e74e47776d0c98bcea8503ed5b4fc87bdacb66.png',\n",
       " '0c0f4358057b2d22484d232292d9d4fa02f264d019c99352e87ab54d.png',\n",
       " '63ed16f6cbfc46f0fa89fb4d4ee760cc9b60ef085dd1df841e7a4dee.png',\n",
       " '49defb3d8f9b3d7bc201cfda782ea08fc785169c767538c83e753e0b.png',\n",
       " '56b5395ed712ba11790b84ac0607f4dfbe14186fd8a8383d4b74cf97.png',\n",
       " '66c344b5fcda1fca13b942a26d2a1244fa63c1be33b35e8c13364d44.png',\n",
       " '73f8aca603019deec3d0717e8bf3acde8eb15f49004697d37f8fdb59.png',\n",
       " '97f748df7f0adf6b79c09ea2df757c8232f29de54ef69efcef743e22.png',\n",
       " '20d0e2702cdf88b0c43ad9d00b3e02d4599f9e27063cb68aea06b98b.png',\n",
       " 'ab21db6bdf0fd29788185d41dcd48f783ab19ccc118db1da7f4493f9.png',\n",
       " 'ee37985b4be70e93a533855194182156fb4bb3967e91cda0a9732154.png',\n",
       " 'b55de6a621e132d58e0e08769899b4ae1aad8c0ef9c07f49a5beffe6.png',\n",
       " '8953ead849efd133ff91e24178a0856ac537ea95cdfcb65e2f01aa49.png',\n",
       " '2c13774f74e46bd2d9ba34c976bf98186524658e8be73ef8f2fbb45f.png',\n",
       " '1a3717d468984dc294a6adad9bf77e47297471c46909d17f2d6d8081.png',\n",
       " 'ea842f756556dcfcaac4e39e1dc767b6de75dd5e03bdd26b50b3e885.png',\n",
       " '1faa18e5b0dee0d566e6c3fb52942fef9b210a87733722065564f53d.png',\n",
       " '0c627fceabca647be4f57534ed25eb2a92dff02d3cc1584cd2d524d5.png',\n",
       " '646fdab6abeec8b1ec74d5eb080096b337b5b7badc0b6a1aaa67141d.png',\n",
       " 'cfe452087515b11809ddac940d7058aca0f3c872b59a7ed2cb64e688.png',\n",
       " 'be00a72acfa9119869f75339bc3fa6b6622d3b29c7e63298418e0aca.png',\n",
       " '039cebf9df844189942640e5d37a009a4b919e17cc510ccb2cbc42d7.png',\n",
       " '544b6d197212d83c99f396cf959a61b1c8f38c29a8361eaa8e28802e.png',\n",
       " '388722ad86dd3a38ef495745e95b7760a1c015c456c5c8dd4283bf51.png',\n",
       " 'ae4bd182f02271720eced49910b5252e73ffa246565b8214ddbbe6c8.png',\n",
       " '78617e0dc086a342e6b97fd158892301c635812ecb210d92cf23344c.png',\n",
       " '0783257e0c0fbf04fc816726813b2363cbb6edb86d51f4e1d1e90ff0.png',\n",
       " '2164792f08096240cfdcfbe8eb8548a9c7b0fb5dfc7e9842f2892fde.png',\n",
       " '82e9d1094b2c60a1bbdecb45d9a251888af4cd96b3a827789e3a4cad.png',\n",
       " 'a38c65e01c250fe15d2ad72dd54064f529d758fde3f28a5e34d92d61.png',\n",
       " '3d33f3de2df4f63f512d058c52e97ea28181471b9f6b5c72af45b18e.png',\n",
       " '9b907a05b91e6f8ab9a7153d602799b909fb6e55b89362f12b2a0220.png',\n",
       " '9e4eb48fbe303dc578685fa85c667b34f0c535dd164d793a46b93e97.png']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_img_name_used['n02056570']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>img_name</th>\n",
       "      <th>is_flickr</th>\n",
       "      <th>dt</th>\n",
       "      <th>worked</th>\n",
       "      <th>class_wnid</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [img_url, img_name, is_flickr, dt, worked, class_wnid, class_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>img_name</th>\n",
       "      <th>is_flickr</th>\n",
       "      <th>dt</th>\n",
       "      <th>worked</th>\n",
       "      <th>class_wnid</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [img_url, img_name, is_flickr, dt, worked, class_wnid, class_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset[df_dataset['img_url']=='http://www.cdli.ca/CITE/emperor_penguins.gif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_url</th>\n",
       "      <th>img_name</th>\n",
       "      <th>is_flickr</th>\n",
       "      <th>dt</th>\n",
       "      <th>worked</th>\n",
       "      <th>class_wnid</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [img_url, img_name, is_flickr, dt, worked, class_wnid, class_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.url_loader) as json_file:\n",
    "    Imagenet_urls_ILSVRC_2016 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_wnid in class_wnids:\n",
    "        list_urls =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
    "        np.random.shuffle(list_urls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = {}\n",
    "for class_wnid in class_wnids:\n",
    "    list_urls[class_wnid] =  Imagenet_urls_ILSVRC_2016[str(class_wnid)]\n",
    "    np.random.shuffle(list_urls[class_wnid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n07720875'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_wnid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://farm2.static.flickr.com/1331/561484939_3ac230bc8f.jpg'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_urls[class_wnid][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://farm4.static.flickr.com/3104/2482335590_2584568f24.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_urls[class_wnid].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test': 'data/test', 'val': 'data/val', 'train': 'data/train'},\n",
       " ['.DS_Store', 'test', 'train', 'val'],\n",
       " ['ant',\n",
       "  'macaque',\n",
       "  'bell pepper',\n",
       "  'cornet',\n",
       "  'king penguin',\n",
       "  'computer keyboard',\n",
       "  'vending machine',\n",
       "  'lionfish',\n",
       "  'sewing machine',\n",
       "  'albatross'],\n",
       " ['n02056570',\n",
       "  'n02058221',\n",
       "  'n02219486',\n",
       "  'n02487347',\n",
       "  'n02643566',\n",
       "  'n03085013',\n",
       "  'n03110669',\n",
       "  'n04179913',\n",
       "  'n04525305',\n",
       "  'n07720875'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "paths, os.listdir('data/'), clean_list(os.listdir(paths[folder])), class_wnids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test king penguin 174\n",
      "test albatross 181\n",
      "test ant 76\n",
      "test macaque 92\n",
      "test lionfish 20\n",
      "test computer keyboard 152\n",
      "test cornet 180\n",
      "test sewing machine 1\n",
      "test vending machine 1\n",
      "test bell pepper 1\n",
      "val king penguin 0\n",
      "val albatross 0\n",
      "val ant 0\n",
      "val macaque 0\n",
      "val lionfish 0\n",
      "val computer keyboard 0\n",
      "val cornet 0\n",
      "val sewing machine 0\n",
      "val vending machine 0\n",
      "val bell pepper 0\n",
      "train king penguin 0\n",
      "train albatross 0\n",
      "train ant 0\n",
      "train macaque 0\n",
      "train lionfish 0\n",
      "train computer keyboard 0\n",
      "train cornet 0\n",
      "train sewing machine 0\n",
      "train vending machine 0\n",
      "train bell pepper 0\n"
     ]
    }
   ],
   "source": [
    "for folder in args.folders:\n",
    "    for class_wnid in class_wnids:\n",
    "        class_name = reverse_id_labels[class_wnid]\n",
    "        class_folder = os.path.join(paths[folder], class_name)\n",
    "        list_dir = os.listdir(class_folder)\n",
    "        print(folder, class_name, len(clean_list(list_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imagenet_urls_ILSVRC_2016.keys()\n",
    "len(Imagenet_urls_ILSVRC_2016['n03085013'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some statistics for the scrapped images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/2021-10-20_dataset_test_inv-ope-de06.json'\n",
    "df_dataset = pd.read_json(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/.local/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py:1616: MatplotlibDeprecationWarning: normalize=None does not normalize if the sum is less than 1 but this behavior is deprecated since 3.3 until two minor releases later. After the deprecation period the default value will be normalize=True. To prevent normalization pass normalize=False \n",
      "  results = ax.pie(y, labels=blabels, **kwds)\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAEyCAYAAABUNHN/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcJ0lEQVR4nO3deZxU1Z338e+vuwFpaVABMeCChB0eUXFQJCIuxHEjJpigRhncgskoMRBHZyYmaBBnxrgMg1seBcQFZ8KTROLGY16CZOIaUJR9EQQmog0I2CLSy5k/ziksqm91V0PX6UI+79eLV9P33Drn3Fu3vvfeU6eqzTknAEB+FTV1BwDgQEDYAkAEhC0AREDYAkAEhC0AREDYAkAEB2TYmtnFZrbQzD43M2dmQ5q4P0NCP0ZFbLPIzMab2ftmVmVmDZ4DaGbTcn2cmXUO2zi+wZ0tQGFbpjVg/cPNbJuZXZvHbmEvmdm9ZrbCzJrlq42cwtbMupjZr81smZntMLNPzGypmT1mZmdkrDvezC7a146Z2Y35CB8z6y5phqRtkq6XdIWkpY3dTkK7ncO+OT7fbeXo7yT9QtIcSVfL7wfkzwRJ5ZKmZhaY2Ugzezuc/D8ys0fMrH1jd8DMDgnH4JCGlO0PGiF3/lXSkZJ+2Dg9qq2kvhXM7CRJr0iqlDRd0mJJLSV1k/RNSZ/Kv2BTfiHpMUm/38e+3ShpraRp+1hPpiHy232jc25BI9ddl87y+2atpHcitpvNUPkTzjWOT7bklZkdKekqSeOcc1UZZT+RdI/8a+zH8i/4sZIGmtkA59xnjdiVQ+SPQUma24Cy/cE+5Y5zbqOZPS3pFjN7IPN5agz1hq38RpRKOt45tzCz0MyOaOxO5Vmqv1sas9Jw+1HsnNvZmPXm0RGStu7vQWtmZc65T5u6H/UYLcnJ31HtZmbt5K9435J0lnOuOix/S9Is+fCdGLerB7THJV0p6VuS/l+j1+6cq/OfpGWSNuWwXmf5A6rWv7R1RsgfROskfSFpk/yZ6LiMuhLrkdQ5lJ8q6QVJGyXtlPQ/kp6XdEo9fUyqc23GNjwu6aPQv9XyB3tpRj3jw2P7yF+VbJBULWlIlnZHZWl7bigfEn4fJf9kLw7tfyDpH7LUeZKk34V9+IWk5ZL+WVJJPftgSJa+TEtbZ7Ckl+SvfD+XtEDS1Ql1TUt/ftOWf0PSn8NjP5I0WVLf0M74jHVN/tZtvqQdkirk75TOyHJ8jQ/H0fxQ/7Q6tvWN0H6tfSLpnFDfjeH3orD/5oXjapf8cfqgpLZZjqWsbSe8hl5LWH5NqOeKhLLVkpbkUHdO/a7jeV9bV1lGWyMk/bf83eyOsH8vzrZvJJ0p6bWw7gZJN4fyQyU9KunjUPaspI51vMYmhW37PLR5VgNz53z5O4dNoY51kn4rqXtGm8Xh+Hs6x+e1jaSektrltH4OFT4XOv+detY7WNLlYd154f+XS7o8bZ0/yYfrz8KBdqekzeHJ65a23uXy41tL0+sJbfSQ9Fk4GP9R/vbsH0M/R9fTx8vDTnbywxSXS7oolB0TnvwvJN0r6UeS/ius+4rSXrBpB8I74WD6SaivR5Z2u0i6Izzm4bTtGZrxQnhd0pqwf64PvztJl2XUd37o5+Kw7aPlD+5qSb+pZx90CG0vDfs41ZeBofxCSVXyL46fh217I/Tjjoy6pikjbCWdLH8CLA/76cawHQuUHLZPhH7/Z9jmcWHdKknDEl5U78jfldwh6VpJI+rY1h+Fx1yQUPak/NDY4eH3gyRtlQ+BcZKuC//fJek9Sc2TAiWH10+HsO6/J5Q9HMq6ZulfjaRW9dSfU79DP24M7f027Xm/qK6ytHYmhPIXwrpj5E+KTtLfJ+ybhfKv7TvlT6apdX8cnt/fhufnnvBc/zGjjvFh/fmS3pQ/Dn8uf2FVKensXHJH0unh+FoY9s/Vkm6Vf02fl7A/50j6MMewHaWEY3pfwnZgeOKcpBWSpoSd1yvL+lkPQkkHJyzrJR8cD2QsX6tw5ZexfExoY0AuG5jw+NST2Dnh4HaZT4Cku8LyqxPqmKt6riTTHjMkPGZUHWV/ldQmbXmpfGi9lrbsIPmz/LzMtsMB6ZTlCjtj3bmqfeVSLH81vVVpVxqSmstfqVZrz5PiNNUO21fD8dI94/FvZh6Ykr4dlv0go44SSX+RP/FYWNY5rFuZ7dhL2MbDwrH1XxnLy+RP2LPSlpmklgl1XB3a/V6ux3nGemeEdccklP0hlCW1+2+hrHs99efc77R9OD5h/brKTgxlExPKfi9pu6SyjH1TI+nkjGPgw7B8UkYd94TH9EhbNj4se0NpJzr5Me0KSUtzeT7S6j48x2PmkbB+rbuZhHVHZdtnSf/qnY3gnHtNUn/5wec28re5D0haYmbzzKxLfXWk1fWZJJnXOoxZlcvfAp+cYzXbws9vmdlBubZdFzMrkjRM0tvOuecziu+UP0C+nfDQ+1zjDqRPdc6ltk/OuR3yV4Xd0tYZKn8lMlXSIWbWLvVPfihF8m9c7o3+ko6WNMU599e0fuySf/EXyY9nJTKzw+VPzs8451ZkPP7ehIdcLn9X8/uM7ThEPog6a89tl6TnnHM5zR5xzm0J9VxoZoekFV0sfyJ7LG1d55z7PGxHcXh3vp2kl8MquR6fmVKzCpLeIygNP79IKNuZsU6iPPY73fflQ+Wx9OcptDNL/uQ1MOMxrznn3kjr5y75E67JDwuk+1P4mflcS9K94bGpejbIXxj1NLNeOfQ99Xoabma5vEe1Ofw8vL4VnXPTnHPmnBufQ725Tf1yzr3nnBvlnOsg/wL4O/kddJqkZ8yseS71mNkJZvas/Atsm3zQlkv6P/LjOLl4WtIfJf2TpC1m9rKZ3Wxmx+T4+CTtJbWSvy3fQ3jBfig/FJBpRcKyffF+wrLNktqm/Z46wKboy/2X+rcslHXYy/aPDT9r7Ye0ZXWdXFNlyxLKliQs6yX/Qv1ItbdlfFgnc1saus8fk78b+F7aspGSPpEP4t3M7Htm9ob8uN4noR+p5yTX4zOTS1WfULYj/GyRUHZQxjpZ5anf6XrJ93+Zaj9Pj4Z1Mp+npGP5k/BzTZblbVVb0ok1dSzlcqE3WdLb8heIW8zseTMbU8fUutTz5LKU77Vckn4PzrkPJE03s8flA3eQpAHyA+dZmdnR8re+2yX9Uv5q9jP5jbpPPuxyaf8LSUPNbID8mxyDJd0uabyZXeac+11Dt2kf1PtCaKDqHNZJHQw3KfsUsr9mWV5oTP4Fe1kd6yzK+L2h+/yF0MZISb8Ox+Hpkh5Kv2Iys+/Ijxu/KT+uuF7+6rJY0ova+w8AlYefhyWUpZ6nTpJWZZR10pdDS1nlsd97NBP6cq6yH6OZJ+isx7ILsy6ytNOonHObzexv5C8Mh8rnxb2SbjOz88Kde7rU81SuRtbgsE1xzrlwNh0kf2DU59vygTrMOTcnvcDM2qr2rVSdZxbn3JvyB5jM7Cj5s9cE+XfoG6pc/mq7T2aBmR0q6Wva97mxjXWmXBl+fuac+2Mj1ZmSuhqptR8k9c5YJ0nqiqVnHY9Pt1JSd0mvO+cqcuphAznnqszsKUk/DkNel8q/qB/LWPUK+ZA6IwzfSJLMLGlbGiIVQkm3yG9J+oH8LXhm2J4iaXkO+6Uh/a7rGKyrbKWkv5W0LtchnEbUS/7NrXS5HIu7hXCfG/7JzI6Tf+PtZ/JvNqfrKmmjc26zGlm9Zz0zG5o01mFmLfXl2GD6LWKFks/iqbPZHmev8PHFpLm6ifWEcaJMG+QDM6ndejnnauRvKU8ws7/NKL5Ffj/t6xVz6kWzV31MM1t+1sQtZpa0f1qaWdle1r1AflrMlenzp8Mc4pvkX5DPZHuwc+4j+THmb4VP6qUe31z+zbtM0+X37Z1J9ZnZ3g6HZEoF60j5cFqePp4YVMtv3+7XhJmZ/AtyrznnyuUD95SE4mfkb/2vN7PitHYvlL9FfjKHJhrS77qOwbrKHg8/J6b3M629xnqekvwkfZgyfEDkMvnnMD34G5IXy+T3+2EZ6xbLT6l8JZeOmVkbM+uZpY1acrmyvVdSWzObJT+VZIeko+Q3uLuk6c6599LWf13S2WZ2s/wL1znnnpa/ndsh6XEzmyw/TjNI0nny07gy+/K6pKvN7Jfy4zapQPyZmX1Tfm7eGvnwvlD+aurfctnoLP5J/jbj92b2gPyVxmD5uYXzVPtKqKGWyF89/8jMdsi/4/+xc+7lOh+VwTn3mZmNlH8XeLmZTQl9PUR+H3xH/i5ibkM76JyrNrPr5U8sb5nZr0OfR8iHxUTn3Mq66pD/9NNcSX82s/vlt/MSJRxrzrmZZjZVPmxOlH9ON8m/4zxQ/ioj5zdg69iut83sPfnAby3/XGeaKWm4pJfNbLqkZvLToup8gypHv5F0q5l9zTn3YVq/ys3sVkm/kvRHM5shf5c4Tj4Q7suh7pz7HW6pV0m6xMxWy4+Vf+ac+0M9ZW+Z/06L8ZLeMbPfyA9vfE3+TdXz5Gcb5EOJpD+FfVMmP7WtpfyspHTZcuf/hoD+//IzbVrKH89l8if7dKfLTyX7TY59+7b8G9W36cv3GLLLYXrDNyXdL38pv0l+Ttxm+floV0kqyli/W9iw7ao9uXiwvpwUvVV+bmxfJU9DOlz+Uxxb5IPWyb85N0R+jGqt/Nlpi/z0kGsUpgnVsz3jU3UllB0rfxb/WH760vuq+0MNteqop+3z5K8ed4bHzw3Lhyj7tLBp6fswbXlf+Tmq/xP6+pH8tKtbJR2WQ19q7fO0stPlP9SwPfT1bTXsQw2DQ192hn7drywfagjrXyE//p9qb638PMwRaet0zvb4HPf9uPD4aklHZVnnWvmT4k75N0V/LX/1U2taUdKyOtruKD9lbVyW8lHyr6+d4dibohynKu1FvwfIT+NLvV+yNpeyUH6+/J3VFvlhv/XyF1HX5bJv6jheah3/2vNDDf+hLz/A9KbC/PSMOhJzR/7iY5b83e8X8nfAr0ganlDH1LD/cp3OOaohx2RqDiOAPDKzh+QvXHo45yqbuj+FLlxJ/0LSsc65tRHaO0L+4uoW51zm1LRGcUB+xSLQBH4uP7XpyqbuCBLdIn/1+2C+Gtjr2QgAcuec+1j+Q0EoQM65G+U/hpw3XNkCQASM2QJABFzZAkAEhC0AREDYAkAEhC0AREDYAkAEhC0ARMCHGoACMn/+/MNLSkoekf8uCS6GsquRtKiqquqa/v37f9zUnckFYQsUkJKSkkeOOOKIXu3bt/+kqKiISfBZ1NTUWHl5ee+NGzc+Iv8nrQoeZ06gsPRt3779doK2bkVFRa59+/bb5O8A9guELVBYigja3IT9tN9k2H7TUQDYnzFmCxSwzrc8178x61v7L+fPr2+dE044oefbb7+d9BeSs5oyZcqhEyZM6Ni+ffvKW2+99cO77767w5w5c1Y9+eSTbRYvXtxy4sSJG5Met3z58uYXXHBBt5UrVyb9ReevFMIWwB4aGrSSNHXq1HYPPvjgB+ecc07Fs88+u/tv4H3/+9/fJmnbvvSnsrJSzZo125cqCgLDCAD2UFpaeoIkffDBB81OOumkHj179uzdrVu3Pi+++GKrpPV/+tOffm3+/PmtRo8e3Xn06NFHppdNmjSp7ciRI4+WpPXr15cMHTr06z169Ojdo0eP3i+99NLB6esuWbKkea9evXq/8sorpZMmTWp75plndj3llFO6n3rqqT3yta0xEbYAEk2ZMuWws846a9uyZcuWLF26dPHJJ5+8I2m9X/3qVx/27dt3x/Tp099/+OGHN2Sr77rrrjv6tNNO+3T58uVLFi9evOTEE0/cmSpbuHBhi+HDh3edMmXKmtNPP32HJC1evLj0mWeeWf3WW28tb/yti49hBACJTjnllM9Gjx7dubKysujiiy/+5NRTT/18X+p79dVXy2bOnLlGkkpKStS2bdvqTZs2FW/ZsqXkoosu6jpz5szV/fv33x3Ap5122vYOHTpU7+t2FAqubAEkOvfccyvmzZu3vFOnTruuuuqqYydPntw2H+2UlZVVd+zYcdecOXP2GKYoLS2tyUd7TYWwBZBoxYoVzY888sjKcePGbRo5cmT5ggULSvelvkGDBn161113tZekqqoqbd68uViSmjVr5l544YXVM2bMaPvQQw8d1hh9L0QMIwAFLJepWvkye/bsskmTJh1RUlLiSktLq5988sk1+1Lfgw8+uG7UqFHHdO/evV1RUZEmT578wVFHHVUpSa1bt66ZPXv2qiFDhnQvKyv7ygwdpONvkAEFZOHChWv79eu3qan7sb9YuHBhu379+nVu6n7kgmEEAIiAYQQAOTvuuON67tq1a4+LtOnTp68ZMGDAPs1UOBAQtgBy9u677zb402XwGEYAgAgIWwCIgLAFgAgIWwCIgDfIgEI2vk2jfp+txm9r9A9JTJo0qe2wYcO2d+7cuTLbOi+++GKr66+//piSkhL31FNPvT9ixIivr1y5cvG8efNKp0yZ0nbatGnrx44d27FVq1bVt99++0eN3cdCwJUtgH3yxBNPtFu3bl2dXzg7ffr0w8aOHfvhsmXLlhx88MG7v/Ng8ODBO6ZNm7Y+17YqK7PmecEjbAHsYfny5c27dOnS55JLLjmma9eufQYNGtStoqLCXn311Zb9+vXr2b17995Dhw79enl5efHUqVMPXbRoUenIkSO79OzZs3dFRYVl1nfPPfe0e+655w674447Og0bNuzY9LJnn3227Iwzzuia+Zi777673eDBg7tVVFTYgAEDelx11VVH9e3bt9eECRM65HPb84mwBVDLunXrDhozZszHq1atWtymTZvq6dOnHzpq1KhjJ06cuGHFihVL+vTp8/nNN9/c8corr/wk9V22y5YtW9KqVatan/8fO3bsprPPPnvrhAkTNsyaNave71eYOHFi++eff77N7NmzV6Xq27Vrly1atGjpbbfdtt8OMTBmC6CWTp06fZH6/toTTjhhx+rVq1t8+umnxeeff36FJF177bWbv/vd73Zp7Haffvrpth07dtw1e/bs1S1atNgd3JdeeumWxm4rNq5sAdTSvHnz3UFXXFzstm7dGuXCrGfPnp9v2LChxZo1a/YYAy4rK9vvv9uWsAVQrzZt2lS3bt26OvV3yB599NG2AwcOrJCkVq1aVW/btq24Mdo5/vjjd9x///0fDBs2rOvatWv3/7/ymIZhBKCQ5WGq1t6aOnXqmh/+8IfHjBkzpujoo4/+YsaMGWslaeTIkZtuuOGGY2666aaav/zlL0uTxm0b4pxzzqm48847N5x77rndXn755RWN0vkCwPfZAgWE77NtGL7PFgCwB4YRADSaoUOHfn39+vUt0pfdcccdG4YPH769qfpUKAhbAI3mpZdeWt3UfShUDCMAhaWmpqam1qewUFvYT/vNlDDCFigsi8rLy9sQuHWrqamx8vLyNpIWNXVfcsUwAlBAqqqqrtm4ceMjGzdu7CsuhupSI2lRVVXVNU3dkVwx9QsAIuDMCQARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARELYAEAFhCwARRAtbM2tpZj1itQcAhSRK2JrZhZLekfRi+P14M5sVo20AKASxrmzHSxogaaskOefekXRspLYBoMnFCttK59y2jGUuUtsA0ORKIrWz2Mwuk1RsZt0kjZH0aqS2AaDJxbqyvUFSH0lfSJohabukGyO1DQBNzpyLdzdvZq0lOefcp9EaBYACEGs2wt+Y2XuS3pX0npktNLP+MdoGgEIQ5crWzN6V9PfOuT+F378h6QHn3HF5bxwACkCsMdvqVNBKknPuvyVVRWobAJpcrCvb+yS1lH9zzEkaIWmnpCckyTm3IO+dAIAmFCts59RR7JxzZ+a9EwDQhGKFbbFzrjrvDQFAgYo1ZrvSzO4ys16R2gOAghIrbPtJWiHpUTN73cx+EObcAsABIeqHGiTJzE6X9JSkQyTNlPRL59yqqJ0AgMhifaih2MyGmdnvJN0n6W5JXST9QdLzMfoAAE0p1hfRrJQ0R9Jdzrn0L6CZaWaDI/UBAJpMrNkI3wgfZEhfNsg59+e8Nw4ABSBW2C5wzp1Y3zIA+KrK6zCCmQ2UdKqk9mY2Nq2otaTifLYNAIUk32O2zSW1Cu2UpS3fLuniPLcNAAUj1jDCMc65D+oo/w/n3A157wgANJEoU7/qCtpgUIx+AEBTifUJMgA4oBG2ABBBoYStNXUHACCfooetmRUlfAnNv8fuBwDEFOu7EZ4ys9ZmdrCkRZKWmNlNqXLn3LQY/QCAphLryra3c267pIskvSDpWElXRGobAJpcrLBtZmbN5MN2lnOuUv5vkQHAASFW2D4saa2kgyXNM7Nj5D9FBgAHhOhfHr67YbMS5xx/zhzAASHfX0Qztp5V7sln+wBQKPL9RTRl9a8CAF99eR9GMLNiSWOcc/fmtSEAKGB5f4PMOVct6dJ8twMAhSzWVyzeK6mZpP+U9FlquXNuQd4bB4ACECts54T/phozSc45d2beGweAAhDrr+vOTVjGhxoAHDBihW1F2v8PknSBpKWR2gaAJtckH2owsxaSZjvnhkRvHACaQFN9n22ppCObqG0AiC7KMIKZvacvx2iLJbWXdHuMtgGgEET767ppv1ZJ+ojvRQBwIGmyL6IBgANJofwNMgD4SiNsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASACwhYAIiBsASCC/wXQDGMv2yta1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAElCAYAAACfw+jgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAK9klEQVR4nO3dT4it913H8c93equomBRkwhXRiHVTDbFNBEsb/0UUhZqKraWNtRVaUBRCFVFcKKgri1axgiA3WO1GMS3+wVXtytBNkxtJbu0uWkEYehc1jRIjt/N1MTPmzvROck45Z5478329YJg5zzkM39WP98zzPL+nujsAAMy0s/QAAAAsRwwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABru09ADAXE8++eRdly5dupLknhz/43Q/ybUbN268//777//8MtMB3J42vXaKQWAxly5dunL58uXX7e7ufmFnZ6ePju/v79f169e/Y29v70qShxYcEeC2s+m102liYEn37O7ufvHmxSxJdnZ2end397kc/NULwHEbXTvFILCknZOL2U1vdKxRALey0bXTQgsAMJgYBAAYTAwCS9rf39+vU96oHNwZB8BxG107xSCwpGvXr1+/8+SidnhH3J1Jri00F8DtbKNrp61lgMXcuHHj/Xt7e1f29vZO3StrodEAblubXjur+5Y3owAAMIDTxAAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBVorBqnpVVf3StocBAOBsrRSD3f2lJO/a8iwAAJyx6u7VPlj1B0leneSvkvz30fHuvrqd0QDOt6p6VZJ/7O4fXHoWgNNcWuOzrz/8/ts3HeskD25sGoALpLu/VFX7VXVndz+39DwAt7JODP5Yd//PzQeq6hs2PA/ARfNfSZ6pqk/k+FmVR5YbCeAl68Tgx6rqrd19I0mq6nKSf0hy/1YmA7gYPn74BXBbWicG/ybJX1fV25N8c5K/S/Ir2xgK4AK51t1P3nygqt6y1DAAJ618A0mSVNUvJvnRJN+a5Oe6+1NbmgvgQqiqq0ne093XDl+/K8kHuvt7lp0M4MArxmBV/fLNL5O8J8nTSZ5Kku7+0NamAzjnqurbkjyW5OEk35uDNfQtbigBbhernCb++hOvP37KcQBO6O5nq+qdObjU5t+T/Eh3v7DsVAAvWes0MQCrqapncrD91pG7kjyX5MUk6e57l5gL4KRVThP/YXd/oKr+PscXtiRJdz+0reEAzququvvl3u/uz53VLAAvZ5XTxB89/P572xwE4CI5ir2qemOSz3T384ev70jyuiRiELgtOE0MsEVV9VSS+/pwsa2qnSRPdPd9y04GcOAV/zN4i+tejnHdC8DLqr7pr+7u3q+qdfZ4BdiqVRakn0rizjeAr8yzVfVIkj85fP0LSZ5dcB6AY1a5geRqd99XVR/t7p85o7kALoSquivJHyV5MAdnWT6Zg02nP7/oYACHVvnP4FdV1cNJ3lRVP3nyze72zE2AUxxG3zuXngPgNKvE4M8n+ekkr0ny4yfe63gAO8CXqapf7e4PVtWHc+ttuR5ZYCyAL/OKMdjdjyd5vKqe6O5HT/tcVf1wd39io9MBnF+fPfz+xKJTALyCjW0tc3Rt4UZ+GQAAZ2KT2xvUBn8XwLl22lObjnh6E3C72GQM2r0a4CWe2gScCzY+BdiO3+zuH6qq3+3uX1t6GIDTrByDVfXV3f3iyxz7t00OBnDOfWNVvSnJQ1X1lzlxKU13X11mLIDjVr6B5FY3iLhpBODWqurtSd6X5IEkn87xGOzufnCRwQBOWOXZxJeTfFOSr6mqN+SlBe2OJF+7xdkAzq3ufizJY1X1G939O6d9rqq+s7s/c4ajARyzyuPo3pvkZ5N8d47vl/V8ko94AgnAV84ZFmBp65wmflt3f2zL8wCMUlVPdfcblp4DmGtnjc9+sqo+VFVPHH79flXdubXJAGawLRewqHVi8NEcnBp+x+HXF5P82TaGAgDgbKyzz+Bru/ttN73+rar65w3PAzDN/y49ADDbOv8ZfKGqHjh6UVVvTvLC5kcCuDiq6s1V9XWHP7/78HKbu4/e7+43LjcdwHo3kLw+yZ8nObpO8AtJ3tvdT29nNIDzr6qeTvJdSe5N8pEkV5K8o7u/f8m5AI6sc5r4s0k+mOS1SV6T5LkkP5FEDAKc7kZ3d1W9Nckfd/ejVfW+pYcCOLJODP5tkv9McjXJf2xlGoCL5/mq+vUk707yfVW1k+TVC88E8P/WOU18rbvv2fI8ABfK4VOcHk7y6e7+p6r6liQ/0N1/sfBoAEnWi8E/TfLh7n5muyMBAHBW1onBf0ny7Un+NcmLOXhGcXf3vdsbD+B8qqrHu/uBqno+xzeWPlo771hoNIBj1onBu291vLs/t9GJAAA4MyvHIAAAF886m04DAHDBiEEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGCw/wO9ACxojpjV3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAElCAYAAACfw+jgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAK9klEQVR4nO3dT4it913H8c93equomBRkwhXRiHVTDbFNBEsb/0UUhZqKraWNtRVaUBRCFVFcKKgri1axgiA3WO1GMS3+wVXtytBNkxtJbu0uWkEYehc1jRIjt/N1MTPmzvROck45Z5478329YJg5zzkM39WP98zzPL+nujsAAMy0s/QAAAAsRwwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABru09ADAXE8++eRdly5dupLknhz/43Q/ybUbN268//777//8MtMB3J42vXaKQWAxly5dunL58uXX7e7ufmFnZ6ePju/v79f169e/Y29v70qShxYcEeC2s+m102liYEn37O7ufvHmxSxJdnZ2end397kc/NULwHEbXTvFILCknZOL2U1vdKxRALey0bXTQgsAMJgYBAAYTAwCS9rf39+vU96oHNwZB8BxG107xSCwpGvXr1+/8+SidnhH3J1Jri00F8DtbKNrp61lgMXcuHHj/Xt7e1f29vZO3StrodEAblubXjur+5Y3owAAMIDTxAAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBxCAAwGBiEABgMDEIADCYGAQAGEwMAgAMJgYBAAYTgwAAg4lBAIDBVorBqnpVVf3StocBAOBsrRSD3f2lJO/a8iwAAJyx6u7VPlj1B0leneSvkvz30fHuvrqd0QDOt6p6VZJ/7O4fXHoWgNNcWuOzrz/8/ts3HeskD25sGoALpLu/VFX7VXVndz+39DwAt7JODP5Yd//PzQeq6hs2PA/ARfNfSZ6pqk/k+FmVR5YbCeAl68Tgx6rqrd19I0mq6nKSf0hy/1YmA7gYPn74BXBbWicG/ybJX1fV25N8c5K/S/Ir2xgK4AK51t1P3nygqt6y1DAAJ618A0mSVNUvJvnRJN+a5Oe6+1NbmgvgQqiqq0ne093XDl+/K8kHuvt7lp0M4MArxmBV/fLNL5O8J8nTSZ5Kku7+0NamAzjnqurbkjyW5OEk35uDNfQtbigBbhernCb++hOvP37KcQBO6O5nq+qdObjU5t+T/Eh3v7DsVAAvWes0MQCrqapncrD91pG7kjyX5MUk6e57l5gL4KRVThP/YXd/oKr+PscXtiRJdz+0reEAzququvvl3u/uz53VLAAvZ5XTxB89/P572xwE4CI5ir2qemOSz3T384ev70jyuiRiELgtOE0MsEVV9VSS+/pwsa2qnSRPdPd9y04GcOAV/zN4i+tejnHdC8DLqr7pr+7u3q+qdfZ4BdiqVRakn0rizjeAr8yzVfVIkj85fP0LSZ5dcB6AY1a5geRqd99XVR/t7p85o7kALoSquivJHyV5MAdnWT6Zg02nP7/oYACHVvnP4FdV1cNJ3lRVP3nyze72zE2AUxxG3zuXngPgNKvE4M8n+ekkr0ny4yfe63gAO8CXqapf7e4PVtWHc+ttuR5ZYCyAL/OKMdjdjyd5vKqe6O5HT/tcVf1wd39io9MBnF+fPfz+xKJTALyCjW0tc3Rt4UZ+GQAAZ2KT2xvUBn8XwLl22lObjnh6E3C72GQM2r0a4CWe2gScCzY+BdiO3+zuH6qq3+3uX1t6GIDTrByDVfXV3f3iyxz7t00OBnDOfWNVvSnJQ1X1lzlxKU13X11mLIDjVr6B5FY3iLhpBODWqurtSd6X5IEkn87xGOzufnCRwQBOWOXZxJeTfFOSr6mqN+SlBe2OJF+7xdkAzq3ufizJY1X1G939O6d9rqq+s7s/c4ajARyzyuPo3pvkZ5N8d47vl/V8ko94AgnAV84ZFmBp65wmflt3f2zL8wCMUlVPdfcblp4DmGtnjc9+sqo+VFVPHH79flXdubXJAGawLRewqHVi8NEcnBp+x+HXF5P82TaGAgDgbKyzz+Bru/ttN73+rar65w3PAzDN/y49ADDbOv8ZfKGqHjh6UVVvTvLC5kcCuDiq6s1V9XWHP7/78HKbu4/e7+43LjcdwHo3kLw+yZ8nObpO8AtJ3tvdT29nNIDzr6qeTvJdSe5N8pEkV5K8o7u/f8m5AI6sc5r4s0k+mOS1SV6T5LkkP5FEDAKc7kZ3d1W9Nckfd/ejVfW+pYcCOLJODP5tkv9McjXJf2xlGoCL5/mq+vUk707yfVW1k+TVC88E8P/WOU18rbvv2fI8ABfK4VOcHk7y6e7+p6r6liQ/0N1/sfBoAEnWi8E/TfLh7n5muyMBAHBW1onBf0ny7Un+NcmLOXhGcXf3vdsbD+B8qqrHu/uBqno+xzeWPlo771hoNIBj1onBu291vLs/t9GJAAA4MyvHIAAAF886m04DAHDBiEEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGAwMQgAMJgYBAAYTAwCAAwmBgEABhODAACDiUEAgMHEIADAYGIQAGCw/wO9ACxojpjV3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for folder in args.folders : \n",
    "    filename = f'results/{datetag}_dataset_{folder}_{args.HOST}.json'\n",
    "    if os.path.isfile(filename):\n",
    "        df_dataset = pd.read_json(filename)\n",
    "\n",
    "        df_type = pd.DataFrame({'urls_type': [len(df_dataset[df_dataset['is_flickr']==1]), \n",
    "                                              len(df_dataset[df_dataset['is_flickr']==0])]},\n",
    "                          index=['is_flickr', 'not_flikr'])\n",
    "        df_flikr = pd.DataFrame({'not_flikr': [df_dataset[df_dataset['is_flickr']==0]['worked'].sum(), \n",
    "                                               (len(df_dataset[df_dataset['is_flickr']==0]) - df_dataset[df_dataset['is_flickr']==0]['worked'].sum())],\n",
    "                                 'is_flickr': [df_dataset[df_dataset['is_flickr']==1]['worked'].sum(), \n",
    "                                               (len(df_dataset[df_dataset['is_flickr']==1]) - df_dataset[df_dataset['is_flickr']==1]['worked'].sum())]},\n",
    "                                  index=['worked', 'not_working'])\n",
    "        df_all = pd.DataFrame({'url': [len(df_dataset[df_dataset['worked']==1]), len(df_dataset[df_dataset['worked']==0])]},\n",
    "                          index=['worked', 'not_working'])\n",
    "\n",
    "        plot_type = df_type.plot.pie(y='urls_type', figsize=(5, 5), labeldistance=None)\n",
    "        plot_type.set_title('Stats for the folder '+ folder + ' (' + str(len(df_dataset)) + ' attempts) :', size = 18)\n",
    "        if not len(df_dataset[df_dataset['is_flickr']==0]) == 0 or len(df_dataset[df_dataset['is_flickr']==1]) == 0: \n",
    "            plot_flickr = df_flikr.plot.pie(subplots=True, figsize=(11, 6), labeldistance=None)\n",
    "        plot_all = df_all.plot.pie(y='url', figsize=(5, 5), labeldistance=None)\n",
    "    else:\n",
    "        print(f'The file {filename} is not available...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : show one representative image from each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning and dataset config\n",
    "\n",
    "In the `model.py` scrip,t we first define the `transform` functions for the datasets. To perform image augmentation, we apply the pyTorch `AutoAugment` function to the `train` and `val` dataset. Then, we load the pretrained models and store them in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'DCNN_transfer_learning/model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DCNN_transfer_learning/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "from DCNN_transfer_learning.init import *\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "\n",
    "# normalization used to train VGG\n",
    "# see https://pytorch.org/hub/pytorch_vision_vgg/\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "transforms_norm = transforms.Normalize(mean=mean, std=std) # to normalize colors on the imagenet dataset\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "from scipy import stats\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "# VGG-16 datasets initialisation\n",
    "def datasets_transforms(image_size=args.image_size, p=0, num_workers=1, batch_size=args.batch_size, **kwargs):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugment\n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
    "            transforms_norm ]),\n",
    "\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.AutoAugment(), # https://pytorch.org/vision/master/transforms.html#torchvision.transforms.AutoAugment\n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
    "            transforms_norm ]),\n",
    "\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((int(image_size), int(image_size))),\n",
    "            transforms.RandomGrayscale(p=p),\n",
    "            transforms.ToTensor(),      # Convert the image to pyTorch Tensor data type.\n",
    "            transforms_norm ]),\n",
    "    }\n",
    "    #print(paths)\n",
    "    \n",
    "\n",
    "    image_datasets = {\n",
    "        folder: datasets.ImageFolder(\n",
    "            paths[folder], \n",
    "            transform=data_transforms[folder]\n",
    "        )\n",
    "        for folder in args.folders\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        folder: torch.utils.data.DataLoader(\n",
    "            image_datasets[folder], batch_size=batch_size,\n",
    "            shuffle=True, num_workers=num_workers\n",
    "        )\n",
    "        for folder in args.folders\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {folder: len(image_datasets[folder]) for folder in args.folders}\n",
    "\n",
    "    return dataset_sizes, dataloaders, image_datasets, data_transforms\n",
    "\n",
    "(dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size)\n",
    "\n",
    "for folder in args.folders : print(f\"Loaded {dataset_sizes[folder]} images under {folder}\")\n",
    "class_names = image_datasets['train'].classes\n",
    "print(\"Classes: \", image_datasets['train'].classes)\n",
    "n_output = len(os.listdir(paths['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes albatross, ant, bell pepper, computer keyboard, cornet, king penguin, lionfish, macaque, sewing machine, vending machine. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolders\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {dataset_sizes[folder]} images under {folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36mdatasets_transforms\u001b[0;34m(image_size, p, num_workers, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     image_datasets = {\n\u001b[0m\u001b[1;32m     47\u001b[0m         folder: datasets.ImageFolder(\n\u001b[1;32m     48\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     image_datasets = {\n\u001b[0;32m---> 47\u001b[0;31m         folder: datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     ):\n\u001b[0;32m--> 310\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes albatross, ant, bell pepper, computer keyboard, cornet, king penguin, lionfish, macaque, sewing machine, vending machine. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.54 s.\n",
      "  System :       0.14 s.\n",
      "Wall time:       0.49 s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "Finaly, we implement the training process in `experiment_train.py`, using a classic training script with pyTorch. For further statistical analyses, we extract factors (like the accuracy and loss) within a `pandas` object (a `DataFrame`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {scriptname}\n",
    "from DCNN_transfer_learning.model import *\n",
    "\n",
    "def train_model(model, num_epochs, dataloaders, lr=args.lr, momentum=args.momentum, beta2=args.beta2, log_interval=100, **kwargs):\n",
    "    \n",
    "    model.to(device)\n",
    "    if beta2 > 0.: \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(momentum, beta2)) #, amsgrad=amsgrad)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum) # to set training variables\n",
    "\n",
    "    df_train = pd.DataFrame([], columns=['epoch', 'avg_loss', 'avg_acc', 'avg_loss_val', 'avg_acc_val', 'device_type']) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "        for i, (images, labels) in enumerate(dataloaders['train']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "        avg_loss = loss_train / dataset_sizes['train']\n",
    "        avg_acc = acc_train / dataset_sizes['train']\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            acc_val = 0\n",
    "            for i, (images, labels) in enumerate(dataloaders['val']):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                acc_val += torch.sum(preds == labels.data)\n",
    "        \n",
    "            avg_loss_val = loss_val / dataset_sizes['val']\n",
    "            avg_acc_val = acc_val / dataset_sizes['val']\n",
    "        \n",
    "        df_train.loc[epoch] = {'epoch':epoch, 'avg_loss':avg_loss, 'avg_acc':float(avg_acc),\n",
    "                               'avg_loss_val':avg_loss_val, 'avg_acc_val':float(avg_acc_val), 'device_type':device.type}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} : train= loss: {avg_loss:.4f} / acc : {avg_acc:.4f} - val= loss : {avg_loss_val:.4f} / acc : {avg_acc_val:.4f}\")\n",
    "\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, df_train\n",
    "\n",
    " \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training and saving the network\n",
    "\n",
    "models_vgg = {}\n",
    "opt = {}\n",
    "#df_train = {}\n",
    "\n",
    "models_vgg['vgg'] = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# Downloading the model\n",
    "model_filenames = {}\n",
    "for model_name in args.model_names:\n",
    "    model_filenames[model_name] = args.model_path + model_name + '.pt'\n",
    "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "\n",
    "    models_vgg[model_name] = torchvision.models.vgg16(pretrained=True)\n",
    "    # TODO : compare with full learning\n",
    "    # Freeze training for all layers\n",
    "    # Newly created modules have require_grad=True by default\n",
    "    for param in models_vgg[model_name].features.parameters():\n",
    "        param.require_grad = False \n",
    "\n",
    "    if model_name == 'vgg16_lin':\n",
    "        num_features = models_vgg[model_name].classifier[-1].out_features\n",
    "        features = list(models_vgg[model_name].classifier.children())\n",
    "        features.extend([nn.Linear(num_features, n_output)]) # Adding one layer on top of last layer\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features)\n",
    "\n",
    "    else : \n",
    "        num_features = models_vgg[model_name].classifier[-1].in_features\n",
    "        features = list(models_vgg[model_name].classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features, n_output)]) # Add our layer with 10 outputs\n",
    "        models_vgg[model_name].classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    if os.path.isfile(model_filenames[model_name]):\n",
    "        print(\"Loading pretrained model for..\", model_name, ' from', model_filenames[model_name])\n",
    "        #print(\"Resume_training : \", resume_training)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name])) #on GPU\n",
    "        else:\n",
    "            models_vgg[model_name].load_state_dict(torch.load(model_filenames[model_name], map_location=torch.device('cpu'))) #on CPU\n",
    "\n",
    "    else:\n",
    "        print(\"Re-training pretrained model...\", model_filenames[model_name])\n",
    "        since = time.time()\n",
    "\n",
    "        p = 1 if model_name == 'vgg16_gray' else 0\n",
    "        if model_name =='vgg16_scale':\n",
    "            df_train = None\n",
    "            for image_size_ in args.image_sizes: # starting with low resolution images \n",
    "                print(f\"Traning {model_name}, image_size = {image_size_}, p (Grayscale) = {p}\")\n",
    "                (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=image_size_, p=p)\n",
    "                models_vgg[model_name], df_train_ = train_model(models_vgg[model_name], num_epochs=args.num_epochs//len(args.image_sizes),\n",
    "                                                             dataloaders=dataloaders)\n",
    "                df_train = df_train_ if df_train is None else df_train.append(df_train_, ignore_index=True)\n",
    "        else :\n",
    "            print(f\"Traning {model_name}, image_size = {args.image_size}, p (Grayscale) = {p}\")\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=p)\n",
    "            models_vgg[model_name], df_train = train_model(models_vgg[model_name], num_epochs=args.num_epochs,\n",
    "                                                        dataloaders=dataloaders)\n",
    "        torch.save(models_vgg[model_name].state_dict(), model_filenames[model_name])\n",
    "        df_train.to_json(filename)\n",
    "        elapsed_time = time.time() - since\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes albatross, ant, bell pepper, computer keyboard, cornet, king penguin, lionfish, macaque, sewing machine, vending machine. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/experiment_train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDCNN_transfer_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolders\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {dataset_sizes[folder]} images under {folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36mdatasets_transforms\u001b[0;34m(image_size, p, num_workers, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     image_datasets = {\n\u001b[0m\u001b[1;32m     47\u001b[0m         folder: datasets.ImageFolder(\n\u001b[1;32m     48\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/DCNN_transfer_learning/model.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     image_datasets = {\n\u001b[0;32m---> 47\u001b[0;31m         folder: datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     ):\n\u001b[0;32m--> 310\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes albatross, ant, bell pepper, computer keyboard, cornet, king penguin, lionfish, macaque, sewing machine, vending machine. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IPython CPU timings (estimated):\n",
      "  User   :       0.02 s.\n",
      "  System :       0.00 s.\n",
      "Wall time:       0.03 s.\n"
     ]
    }
   ],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display both average accuracy and loss during the training phase and during the validation one : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'results/2021-10-26_inv-ope-de06_train_vgg16_lin.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "model_name = 'vgg16_lin'\n",
    "filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "%ls {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/quantic/science/FastAndCurious/2021-04-28_transfer_learning/experiment_train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'results/{datetag}_{args.HOST}_train_{model_name}.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# plt.xticks(fontsize=18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "for model_name in args.model_names:\n",
    "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "    df_train = pd.read_json(filename)\n",
    "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi//2))\n",
    "    # plt.xticks(fontsize=18)\n",
    "    # plt.yticks(fontsize=18)\n",
    "    ax = df_train['avg_loss'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax = df_train['avg_loss_val'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax.legend([\"avg_loss\", \"avg_loss_val\"], fontsize=18);\n",
    "    ax.set_xlabel(\"Epoch\", size=18)\n",
    "    ax.set_ylabel(\"Loss value\", size=18)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    axs.set_title(f'Average values of the loss by epoch : {filename}' , size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in args.model_names:\n",
    "    filename = f'results/{datetag}_{args.HOST}_train_{model_name}.json'\n",
    "    df_train = pd.read_json(filename)\n",
    "    fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi//2))\n",
    "    # plt.xticks(fontsize=18)\n",
    "    # plt.yticks(fontsize=18)\n",
    "    ax = df_train['avg_acc'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax = df_train['avg_acc_val'].plot(lw=2, marker='.', markersize=10)\n",
    "    ax.legend([\"avg_acc\", \"avg_acc_val\"], fontsize=18);\n",
    "    ax.set_xlabel(\"Epoch\", size=18)\n",
    "    ax.set_ylabel(\"Accuracy value\", size=18)\n",
    "    ax.set_ylim(0.60, 1)\n",
    "    axs.set_title(f'Average values of the accuracy by epoch : {filename}' , size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Image processing and recognition for differents labels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networks are now ready for a quantitative evaluation. The second part of this notebook offers a comparison between : \n",
    "\n",
    "- A pre-trained image recognition's networks, here VGG, trained on the [Imagenet](http://image-net.org/) dataset wich allows to work on naturals images for $1000$ labels, taken from the `torchvision.models` library\n",
    "\n",
    "- And four re-trained version of the same network VGG16 based on a reduced Imagenet dataset wich allows to focus on naturals images from $10$ labels.\n",
    "\n",
    "For further statistical analyses, we extract these differents factors (like the accuracy and the processing time for differents datasets at differents resolution) in a `pandas` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_basic.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_transfer_learning.model import *\n",
    "#from experiment_train import *\n",
    "filename = f'results/{datetag}_results_1_{args.HOST}.json'\n",
    "print(f'{filename=}')\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device_type', 'top_1']) \n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, batch_size=1)\n",
    "        \n",
    "        for i_image, (data, label) in enumerate(dataloaders['test']):            \n",
    "            data, label = data.to(device), label.to(device)\n",
    "            \n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name]\n",
    "                model = model.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    _, indices = torch.sort(out, descending=True)\n",
    "                    if model_name == 'vgg' : # our previous work\n",
    "                        top_1 = labels[indices[0]]\n",
    "                        percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
    "                        perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
    "                    else :\n",
    "                        top_1 = subset_labels[indices[0]] \n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
    "                        perf_ = percentage[label].item()\n",
    "                    elapsed_time = time.time() - tic\n",
    "                    \n",
    "                print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {elapsed_time:.3f} seconds, best confidence for : {top_1}')\n",
    "                df.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':elapsed_time, 'fps': 1/elapsed_time,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'device_type':device.type, 'top_1':top_1}\n",
    "                i_trial += 1\n",
    "        df.to_json(filename)\n",
    "\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect our results, we can already display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_1_{args.HOST}.json'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall\n",
    "\n",
    "We now compute the top-1 accuracy (which is a metric that describes how the model performs across all classes, here top 1 because we only take the best likelihood at the outputof the networks), the precision (which reflects how reliable the model is in classifying samples as Positive) and the recall (which measures the model's ability to detect Positive samples) of each networks. We use the [sklearn librairy](https://sklearn.org/index.html) to perform this [analysis](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precision = pd.DataFrame({model_name: {subset_label: precision_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
    "                                                                  df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"],\n",
    "                                                                 average='micro')\n",
    "                                    for subset_label in subset_labels} \n",
    "                       for model_name in models_vgg.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_precision.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title('Precision for each models - experiment 1', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.set_xlabel('Models', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : use the f1-score everywhere\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_score = pd.DataFrame({model_name: {subset_label: f1_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
    "                                                                df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"],\n",
    "                                                                average='micro')\n",
    "                                    for subset_label in subset_labels} \n",
    "                       for model_name in models_vgg.keys()})\n",
    "\n",
    "ax = df_f1_score.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title('F1-score for each models - experiment 1', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
    "                                                                 df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"])\n",
    "                                    for subset_label in subset_labels} \n",
    "                       for model_name in models_vgg.keys()})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title('Accuracy top_1 : for each models - experiment 1', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.set_xlabel('Models', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({'accuracy': [accuracy_score(df[df['model']==model_name][\"top_1\"], df[df['model']==model_name][\"label\"]) for model_name in models_vgg.keys()]}, index=models_vgg.keys())\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=14, fmt='%.3f')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title('Average accuracy top_1 : for each models - experiment 1', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.set_xlabel('Models', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time\n",
    "\n",
    "A display of the differents computation time of each models on the same dataset for the sequence of trials :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    df[df['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=model_name,ax=ax, color=color, density=True)\n",
    "    ax.legend(loc='upper left', fontsize=20)\n",
    "    ax.set_xlim(df['time'].quantile(.01), df['time'].quantile(.99))\n",
    "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
    "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification likelihood\n",
    "\n",
    "This graph shows the frequency of the logit of the classification likelihood for our four models and the pyTorch VGG16 model. The classification likelihood represent a predicted likelihood probability of detection for a given label at the output of the network. As most of them are close either to 100 as to 0, we applied a logit function to make the difference between these networks more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    (logit(df[df['model']==model_name]['perf']/100)/np.log2(10)).plot.hist(bins=np.linspace(0, 5, 150), lw=1, label=model_name, ax=ax, color=color, density=True)\n",
    "    #df[df['model']==model_name]['perf'].plot.hist(bins=np.linspace(99.95, 100, 150), lw=1, label=model_name, ax=ax, color=color, density=True)\n",
    "    ax.legend(loc='upper left', fontsize=20)\n",
    "    ax.set_ylim(0, 5)\n",
    "    # ax.tick_params(axis='x', labelsize=14)\n",
    "    # ax.tick_params(axis='y', labelsize=14)\n",
    "axs[-1].set_xlabel('Classification likelihood (ban)', size=18)\n",
    "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : change name of variable perf to likelihood ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO \n",
    "- voir si on peut donner la valeur du logit en hartley base 10 https://en.wikipedia.org/wiki/Hartley_(unit) tracer le chance level\n",
    "- show success of categorization vs likelihood / odd-ratio of label vs rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display\n",
    "\n",
    "Here we display the 64 *worsts* Classification likelihood's, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
    "for i_image, idx in enumerate(df[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i_image//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df.loc[idx]['label'] + ' | ' + df.loc[idx]['model'], color='g')\n",
    "    perf_ = df.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "To make it even clearer we extracted a specific mean for each models : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    mean_acc = (df[df['model']==model_name][\"top_1\"] == df[df['model']==model_name][\"label\"]).mean()\n",
    "    print(f'For the {model_name} model, the mean accuracy = {mean_acc*100:.4f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification likelihood's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf = np.mean(df[df['model']==model_name][\"perf\"])\n",
    "    print(f'For the {model_name} model, the mean clasification likelihood = {med_perf:.4f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time 's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf = np.mean(df[df['model']==model_name][\"time\"])\n",
    "    print(f'For the {model_name} model, the mean computation time = {med_perf:.5f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame per second's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf = np.mean(df[df['model']==model_name][\"fps\"])\n",
    "    print(f'For the {model_name} model, the mean fps = {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Image processing and recognition for differents resolutions :\n",
    "\n",
    "Let's now study that same likelihood indicators at different image resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_downsample.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "#import model's script and set the output file\n",
    "from DCNN_transfer_learning.model import *\n",
    "filename = f'results/{datetag}_results_2_{args.HOST}.json'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_downsample = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_downsample = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'image_size', 'filename', 'device_type', 'top_1']) \n",
    "        # image preprocessing\n",
    "        for image_size_ in args.image_sizes:\n",
    "            (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=image_size_, batch_size=1)\n",
    "            print(f'Résolution de {image_size_=}')\n",
    "            # Displays the input image of the model \n",
    "            for i_image, (data, label) in enumerate(dataloaders['test']):                \n",
    "                data, label = data.to(device), label.to(device)\n",
    "\n",
    "                for model_name in models_vgg.keys():\n",
    "                    model = models_vgg[model_name]\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
    "                        tic = time.time()\n",
    "                        out = model(data).squeeze(0)\n",
    "                        _, indices = torch.sort(out, descending=True)\n",
    "                        if model_name == 'vgg' : # our previous work\n",
    "                            top_1 = labels[indices[0]]\n",
    "                            percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
    "                            perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
    "                        else :\n",
    "                            top_1 = subset_labels[indices[0]] \n",
    "                            percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
    "                            perf_ = percentage[label].item()\n",
    "                        dt = time.time() - tic\n",
    "                    print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds, best confidence for : {top_1}')\n",
    "                    df_downsample.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                       'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                       'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'image_size': image_size_, 'device_type':device.type, 'top_1':str(top_1)}\n",
    "                    i_trial += 1\n",
    "\n",
    "            df_downsample.to_json(filename)\n",
    "\n",
    "main()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, again, we collect our results, and display all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_2_{args.HOST}.json'\n",
    "#filename = 'results/2021-04-20_results_2_INV-133-DE01.json'\n",
    "df_downsample = pd.read_json(filename)\n",
    "df_downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "And extract the accuracy, the precision ans the recall for each networks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_size in args.image_sizes:\n",
    "    pprint(f'Benchmarking image size = {image_size}')\n",
    "    df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"top_1\"], \n",
    "                                                                     df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"label\"])\n",
    "                                        for subset_label in subset_labels} \n",
    "                           for model_name in models_vgg.keys()})\n",
    "\n",
    "    ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "    plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "    ax.set_title(f'Experiment 2 - image size = {image_size}', size=20)\n",
    "    ax.set_ylabel('Accuracy top_1', size=14)\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {image_size: accuracy_score(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"top_1\"], \n",
    "                                                               df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"label\"])\n",
    "                                    for image_size in args.image_sizes} \n",
    "                       for model_name in models_vgg.keys()})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title(f'Experiment 2 - image sizes = {args.image_sizes}', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time\n",
    "\n",
    "A display of the differents computation time of each models on the same dataset for differents resolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "for color, model_name in zip(colors, models_vgg.keys()):\n",
    "    axs = sns.violinplot(x=\"image_size\", y=\"time\", data=df_downsample, inner=\"quartile\", hue='model')\n",
    "    axs.set_title('Processed on : ' + args.HOST + '_' + str(df_downsample['device_type'][0]), size = 20)\n",
    "    axs.set_ylabel('Computation time (s)', size=18)\n",
    "    axs.set_xlabel('Trial', size=18)\n",
    "    axs.set_yscale('log')\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:5], l[:5], loc='upper center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification likelihood\n",
    "\n",
    "Let's display the likelihood of each models on the same dataset for differents resolutions. Here accuracies are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi))\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "axs = sns.violinplot(x=\"image_size\", y=\"perf\", data=df_downsample, inner=\"quartile\", hue='model', cut = 0, scale = 'width')\n",
    "axs.set_title('Processed on : ' + args.HOST + '_' + str(df_downsample['device_type'][0]), size=20)\n",
    "axs.set_ylabel('Classification likelihood (%)', size=18)\n",
    "axs.set_xlabel('Image size', size=18)\n",
    "h, l = axs.get_legend_handles_labels()\n",
    "axs.legend(h[:5], l[:5], loc ='center', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display\n",
    "\n",
    "The 64 worsts classification likelihood, all models and sizes combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
    "for i, idx in enumerate(df_downsample[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df_downsample.loc[idx]['i_image']][0]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_downsample.loc[idx]['label'] + ' | ' + df_downsample.loc[idx]['model']+ ' | ' + str(df_downsample.loc[idx]['image_size']), color='g')\n",
    "    perf_ = df_downsample.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "Again, we extracted a specific mean for each models : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    pprint(f'Benchmarking model {model_name}')\n",
    "    for image_size in args.image_sizes:\n",
    "        df_ = df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)]\n",
    "        mean_acc = (df_[\"top_1\"] == df_[\"label\"]).mean()\n",
    "        print(f'For size {image_size}, the mean accuracy = {mean_acc*100:.4f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification likelihood's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    pprint(f'Benchmarking model {model_name}')\n",
    "    for image_size in args.image_sizes:\n",
    "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"perf\"])\n",
    "        print(f'For size {image_size}, the mean clasification likelihood = {med_perf:.5f} %' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time 's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    pprint(f'Benchmarking model {model_name}')\n",
    "    for image_size in args.image_sizes:\n",
    "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"time\"])\n",
    "        print(f'For size {image_size}, the mean computation time = {med_perf:.3f} s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame per second's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    pprint(f'Benchmarking model {model_name}')\n",
    "    for image_size in args.image_sizes:\n",
    "        med_perf = np.mean(df_downsample[(df_downsample['model']==model_name) & (df_downsample['image_size']==image_size)][\"fps\"])\n",
    "        print(f'For size {image_size}, the mean fps = {med_perf:.3f} Hz' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Image processing and recognition on grayscale images :\n",
    "\n",
    "Again, same likelihood indicators but now with a grayscale transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_grayscale.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_transfer_learning.model import *\n",
    "filename = f'results/{datetag}_results_3_{args.HOST}.json'\n",
    "\n",
    "def main():\n",
    "    if os.path.isfile(filename):\n",
    "        df_gray = pd.read_json(filename)\n",
    "    else:\n",
    "        i_trial = 0\n",
    "        df_gray = pd.DataFrame([], columns=['model', 'perf', 'fps', 'time', 'label', 'i_label', 'i_image', 'filename', 'device_type', 'top_1']) \n",
    "        # image preprocessing setting a grayscale output\n",
    "        (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=1, batch_size=1)\n",
    "\n",
    "        # Displays the input image of the model \n",
    "        for i_image, (data, label) in enumerate(dataloaders['test']):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            for model_name in models_vgg.keys():\n",
    "                model = models_vgg[model_name]\n",
    "                model = model.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    i_label_top = reverse_labels[image_datasets['test'].classes[label]]\n",
    "                    tic = time.time()\n",
    "                    out = model(data).squeeze(0)\n",
    "                    if model_name == 'vgg' :\n",
    "                        percentage = torch.nn.functional.softmax(out[args.subset_i_labels], dim=0) * 100\n",
    "                        _, indices = torch.sort(out, descending=True)\n",
    "                        top_1 = labels[indices[0]]\n",
    "                        perf_ = percentage[reverse_subset_i_labels[i_label_top]].item()\n",
    "                    else :\n",
    "                        percentage = torch.nn.functional.softmax(out, dim=0) * 100\n",
    "                        _, indices = torch.sort(out, descending=True)\n",
    "                        top_1 = subset_labels[indices[0]] \n",
    "                        perf_ = percentage[label].item()\n",
    "                dt = time.time() - tic\n",
    "                df_gray.loc[i_trial] = {'model':model_name, 'perf':perf_, 'time':dt, 'fps': 1/dt,\n",
    "                                   'label':labels[i_label_top], 'i_label':i_label_top, \n",
    "                                   'i_image':i_image, 'filename':image_datasets['test'].imgs[i_image][0], 'device_type':device.type, 'top_1':str(top_1)}\n",
    "                print(f'The {model_name} model get {labels[i_label_top]} at {perf_:.2f} % confidence in {dt:.3f} seconds, best confidence for : {top_1}')\n",
    "                i_trial += 1\n",
    "        df_gray.to_json(filename)\n",
    "\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Collecting all the results, displaying all the data in a table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'results/{datetag}_results_3_{args.HOST}.json'\n",
    "#filename = 'results/2021-04-20_results_3_INV-133-DE01.json'\n",
    "df_gray = pd.read_json(filename)\n",
    "df_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_gray['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display\n",
    "\n",
    "The 64 worsts classification likelihood, all model combined : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_image_i = 8\n",
    "N_image_j = 8\n",
    "\n",
    "fig, axs = plt.subplots(N_image_i, N_image_j, figsize=(fig_width, fig_width))\n",
    "for i, idx in enumerate(df_gray[\"perf\"].argsort()[:(N_image_i*N_image_j)]):\n",
    "    ax = axs[i%N_image_i][i//N_image_i]\n",
    "    ax.imshow(imageio.imread(image_datasets['test'].imgs[df_gray.loc[idx]['i_image']][0], pilmode=\"L\"), cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(df_gray.loc[idx]['label'] + ' | ' + df_gray.loc[idx]['model'], color='g')\n",
    "    perf_ = df_gray.loc[idx]['perf']\n",
    "    ax.set_ylabel(f'P={perf_:2.3f}%', color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification likelihood compared with experiment 1\n",
    "\n",
    "Let's analyze the classification likelihood of each models on the same dataset for color versus grayscale images. Here likelihood's are displayed as a violin plot to allow a better representation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(fig_width, fig_width/phi**2))\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "for color, df_, label in zip(['gray', 'red'], [df_gray, df], ['black', 'color']):\n",
    "    axs = sns.violinplot(x=\"model\", y=\"perf\", data=df_, inner=\"quartile\", cut=0, color=color, alpha=.5, scale = 'width')\n",
    "axs.set_title('Processed on : ' + args.HOST + '_' + str(df_['device_type'][0]), size=20)\n",
    "axs.set_ylabel('Classification likelihood (%)', size=18)\n",
    "axs.set_xlabel('Model', size=18)\n",
    "axs.legend(['Color', 'Gray'], fontsize=18)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification likelihood's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf_orig = np.mean(df[df['model']==model_name][\"perf\"])\n",
    "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"perf\"])\n",
    "    print(f'For the {model_name} model, the mean clasification likelihood = {med_perf:.5f} % (color = {med_perf_orig:.5f} % )' )\n",
    "    print(stats.ttest_1samp(df_gray[df_gray['model']==model_name][\"perf\"], np.mean(df[df['model']==model_name][\"perf\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the learned models on grayscale images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = {} \n",
    "results['acc'] = []\n",
    "results['precision'] = {}\n",
    "results['recall'] = {}\n",
    "\n",
    "for model_name in models_vgg.keys():\n",
    "    #pprint('Model : ' + model_name)\n",
    "    model_pred = df_gray[df_gray['model']==model_name][\"top_1\"].values.tolist()\n",
    "    ground_truth = df_gray[df_gray['model']==model_name][\"label\"].values.tolist()\n",
    "    acc = sklearn.metrics.accuracy_score(ground_truth, model_pred)\n",
    "    results['acc'].append(acc)\n",
    "    \n",
    "    #print(f'Accuracy top_1 : {acc:.3f}')\n",
    "    pres = sklearn.metrics.multilabel_confusion_matrix(ground_truth, model_pred)\n",
    "    results['precision'][model_name] = []\n",
    "    #pprint('Precision :')\n",
    "    for i, label in zip(pres, subset_labels) :\n",
    "        pres_ = np.flip(i)\n",
    "        pres_ = ((pres_[0][0]) / (pres_[0][0] + pres_[-1][0]))\n",
    "        results['precision'][model_name].append(pres_)\n",
    "        #print(f'{pres_:.3f} for the label {label}')\n",
    "    results['recall'][model_name] = []\n",
    "    #pprint('Recall :')\n",
    "    for i, label in zip(pres, subset_labels) :\n",
    "        pres_ = np.flip(i)\n",
    "        pres_ = ((pres_[0][0]) / (pres_[0][0] + pres_[0][-1]))\n",
    "        results['recall'][model_name].append(pres_)\n",
    "        #print(f'{pres_:.3f} for the label {label}')\n",
    "\n",
    "for mode in ['recall', 'precision']:\n",
    "    df_acc = pd.DataFrame({'vgg16_gray': results[mode]['vgg16_gray'],\n",
    "                        'vgg16_lin': results[mode]['vgg16_lin'],\n",
    "                         'vgg16_gen': results[mode]['vgg16_gen'],\n",
    "                          'vgg16_scale': results[mode]['vgg16_scale'],\n",
    "                           'vgg': results[mode]['vgg'],\n",
    "                      }, index=subset_labels)\n",
    "    ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize = 10)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "    ax.set_title(mode.capitalize()+' for each models - experiment 3', size=20)\n",
    "    ax.set_ylabel(mode.capitalize(), size=14)\n",
    "    ax.set_xlabel('Labels', size=16)\n",
    "    \n",
    "df_acc = pd.DataFrame({'accuracy': results['acc']},\n",
    "                  index = models_vgg.keys())\n",
    "ax = df_acc.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize = 18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "ax.bar_label(ax.containers[0], padding=-24, color='black', fontsize=14, fmt='%.3f')\n",
    "ax.set_title('Accuracy top_1 : for each models - experiment 3', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.set_xlabel('Models', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    mean_acc_orig = (df[df['model']==model_name][\"top_1\"] == df[df['model']==model_name][\"label\"]).mean()\n",
    "    mean_acc = (df_gray[df_gray['model']==model_name][\"top_1\"] == df_gray[df_gray['model']==model_name][\"label\"]).mean()\n",
    "    print(f'For the {model_name} model, the mean clasification likelihood = {mean_acc*100:.5f} % (color = {mean_acc_orig*100:.5f} % )' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {subset_label: accuracy_score(df[(df['model']==model_name) & (df['label']==subset_label)][\"top_1\"], \n",
    "                                                                 df[(df['model']==model_name) & (df['label']==subset_label)][\"label\"])\n",
    "                                    for subset_label in subset_labels} \n",
    "                       for model_name in models_vgg.keys()})\n",
    "\n",
    "ax = df_acc.plot.bar(rot=60, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(subset_labels)-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title('Accuracy top_1 : for each models - experiment 1', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.set_xlabel('Models', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({model_name: {label: accuracy_score(df_[(df_['model']==model_name)][\"top_1\"], \n",
    "                                                               df_[(df_['model']==model_name)][\"label\"])\n",
    "                                    for label, df_ in zip(['original', 'gray'], [df, df_gray])} \n",
    "                       for model_name in models_vgg.keys()})\n",
    "\n",
    "ax = df_acc.T.plot.bar(rot=0, figsize=(fig_width, fig_width//4), fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.hlines(xmin=-.5, xmax=len(models_vgg.keys())-.5, y=1/n_output, ls='--', ec='k', label='chance level')\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_label_demo.html\n",
    "for container in ax.containers: ax.bar_label(container, padding=-50, color='black', fontsize=14, fmt='%.3f', rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.1, .5), loc='lower right')\n",
    "ax.set_title(f'Experiment 3 - color vs gray images', size=20)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : analyse *post-hoc* effect of contrast or salience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time\n",
    "\n",
    "A display of the differents computation time of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gray['time'].min(), df_gray['time'].max(), df_gray['time'].quantile(.005), len(df_gray[df_gray['model']==model_name]['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time 's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf_orig = np.mean(df[df['model']==model_name][\"time\"])\n",
    "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"time\"])\n",
    "    print(f'For the {model_name} model, the mean computation time = {med_perf:.4f} s (color = {med_perf_orig:.4f} s )' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame per second's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_vgg.keys():\n",
    "    med_perf_orig = np.mean(df[df['model']==model_name][\"fps\"])\n",
    "    med_perf = np.mean(df_gray[df_gray['model']==model_name][\"fps\"])\n",
    "    print(f'For the {model_name} model, the mean fps = {med_perf:.3f} Hz (color = {med_perf_orig:.3f} Hz )' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2))\n",
    "for color, df_, label, legend in zip(['gray', 'red'], [df_gray, df], ['black', 'color'], ['Grayscale', 'Regular']):\n",
    "    for ax, model_name in zip(axs, models_vgg.keys()):\n",
    "        ax.set_ylabel('Frequency', fontsize=14) \n",
    "        df_[df_['model']==model_name]['time'].plot.hist(bins=150, lw=1, label=str(legend+ ' ' + model_name), ax=ax, color=color, density=True)\n",
    "        ax.legend(loc='upper right', fontsize=20)\n",
    "        ax.set_xlim(df_gray['time'].quantile(.01), df_gray['time'].quantile(.99))\n",
    "axs[-1].set_xlabel('Processing time (s)', size=18)\n",
    "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification likelihood\n",
    "\n",
    "A display of the classification likelihood of each models on the same dataset for a single resolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(models_vgg.keys()), 1, figsize=(fig_width, fig_width*phi//2), sharex=True, sharey=True)\n",
    "# plt.xticks(fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "for ax, color, model_name in zip(axs, colors, models_vgg.keys()):\n",
    "    ax.set_ylabel('Frequency', fontsize=14)\n",
    "    df_gray[df_gray['model']==model_name]['perf'].plot.hist(bins=np.linspace(90, 100, 100), lw=0, alpha=0.3, label=model_name + '_gray', ax=ax, color='k', density=True)\n",
    "    df[df['model']==model_name]['perf'].plot.hist(bins=np.linspace(90, 100, 100), lw=0, alpha=0.3, label=model_name + '_color', ax=ax, color=color, density=True)\n",
    "    ax.legend(loc='upper left', fontsize=20)\n",
    "    ax.set_xlim(90, 100)\n",
    "    ax.set_ylim(0, 1)\n",
    "    # ax.tick_params(axis='x', labelsize=14)\n",
    "    # ax.tick_params(axis='y', labelsize=14)\n",
    "axs[-1].set_xlabel('Classification likelihood (%)', size=18)\n",
    "axs[0].set_title('Processed on : ' + args.HOST + '_' + str(df['device_type'][0]), size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "\n",
    "As a summary, we have shown here that one can implement transfer learning on a subset of images and that it reaches a higher accuracy than a simpler hypothesis. We have also shown that transfer learning could also be used to teach the network to different perturbations of the image, for instance changes in the resolution of in the colorspace. Such framework is thus promising for further applications which aim at modelling higher-order cognitive processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: scan of some parameters\n",
    "\n",
    "If there is some GPU time time left, let's try to *meta*-optimize some parameters by testing how accuracy would vary. To avoid potential over-fitting problems, we perform that test on the `val` validation set which is separate from the `test` and `train` datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptname = 'experiment_scan.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scriptname}\n",
    "\n",
    "#import model's script and set the output file\n",
    "from DCNN_transfer_learning.model import *\n",
    "from experiment_train import train_model\n",
    "\n",
    "scan_dicts= {'batch_size' : [8, 13, 21, 34, 55],\n",
    "             'lr': args.lr * np.logspace(-1, 1, 7, base=10),\n",
    "             'momentum': 1 - np.logspace(-3, -.5, 7, base=10),\n",
    "             'beta2': 1 - np.logspace(-5, -1, 7, base=10),\n",
    "            }\n",
    "\n",
    "def main(N_avg=10):\n",
    "\n",
    "    for key in scan_dicts:\n",
    "        filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
    "        print(f'{filename=}')\n",
    "        if os.path.isfile(filename):\n",
    "            df_scan = pd.read_json(filename)\n",
    "        else:\n",
    "            i_trial = 0\n",
    "            measure_columns = [key, 'avg_loss_val', 'avg_acc_val', 'time']\n",
    "\n",
    "            df_scan = pd.DataFrame([], columns=measure_columns) \n",
    "            for i_trial, value in enumerate(scan_dicts[key]):\n",
    "                new_kwarg = {key: value}\n",
    "                print('trial', i_trial, ' /', len(scan_dicts[key]))\n",
    "                print('new_kwarg', new_kwarg)\n",
    "                # Training and saving the network\n",
    "                models_vgg_ = torchvision.models.vgg16(pretrained=True)\n",
    "                # Freeze training for all layers\n",
    "                # Newly created modules have require_grad=True by default\n",
    "                for param in models_vgg_.features.parameters():\n",
    "                    param.require_grad = False \n",
    "\n",
    "                num_features = models_vgg_.classifier[-1].in_features\n",
    "                features = list(models_vgg_.classifier.children())[:-1] # Remove last layer\n",
    "                features.extend([nn.Linear(num_features, n_output)]) # Add our layer with `n_output` outputs\n",
    "                models_vgg_.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "                since = time.time()\n",
    "\n",
    "                (dataset_sizes, dataloaders, image_datasets, data_transforms) = datasets_transforms(image_size=args.image_size, p=0, **new_kwarg)\n",
    "                models_vgg_, df_train = train_model(models_vgg_, num_epochs=args.num_epochs//4, dataloaders=dataloaders, **new_kwarg)\n",
    "\n",
    "                elapsed_time = time.time() - since\n",
    "                print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "\n",
    "                df_scan.loc[i_trial] = {key:value, 'avg_loss_val':df_train.iloc[-N_avg:-1]['avg_loss_val'].mean(), \n",
    "                                   'avg_acc_val':df_train.iloc[-N_avg:-1]['avg_acc_val'].mean(), 'time':elapsed_time}\n",
    "                print(df_scan.loc[i_trial])\n",
    "                i_trial += 1\n",
    "            df_scan.to_json(filename)\n",
    "\n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -int {scriptname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for key in scan_dicts:\n",
    "    filename = f'results/{datetag}_train_scan_{key}_{args.HOST}.json'\n",
    "    df_scan = pd.read_json(filename)\n",
    "    print(df_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
